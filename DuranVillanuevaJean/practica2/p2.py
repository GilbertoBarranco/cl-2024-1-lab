# -*- coding: utf-8 -*-
"""Copia de p2_lc.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14nAqAmIdbwGvdIBRaRiXxbpJgrzyUcOT

# Indicaciones
- Obtener 10 oraciones al azar del conjunto de pruebas del corpus SIGMORPH 2022
    - Track: Sentences ✅
    - Guardarlas en un objeto pandas [OPCIONAL]
- Usar las bibliotecas spacy y nltk para realizar los siguientes procesos:
    - Stemming (nltk) ✅
    - Lemmatization (spacy) ✅
    - Obtención de información morfologica (spacy) ✅
    - Imprimir la información en pantalla (formato libre) ✅
- ¿Para qué lenguas? La intersección de las siguientes lenguas:
    - Lenguas disponibles como [modelos pre-entrenados en spacy](https://spacy.io/usage/models)
    - Lenguas disponibles para [stemming en nltk](https://www.nltk.org/api/nltk.stem.snowball.html?highlight=snowball#module-nltk.stem.snowball)
    - Lenguas disponibles en el shared task, track: sentences

**NOTAS**:
- Entregar un archivo `.py` (lo pueden exportar del notebook)
- Incluyan la siguiente información en un `README.md`:
    - Cómo correr el script
    - Que dependencias son necesarias y como instalarlas

# Importaciones y descargas
"""

import random

import pandas as pd

import spacy 

import requests

import nltk

#!pip install nltk

#!python -m spacy download en_core_web_sm

if __name__ == '__main__':

  """# Funciones

  ## Tomando 10 frases como corpus
  """

  def get_files(lang: str, track: str = "word") -> list[str]:
      """Genera una lista de nombres de archivo basados en el idioma y el track

      Parameters:
      ----------
      lang : str
          Idioma para el cual se generarán los nombres de archivo.
      track : str, optional
          Track del shared task de donde vienen los datos (por defecto es "word").

      Returns:
      -------
      list of str
          Una lista de nombres de archivo generados para el idioma y la pista especificados.
      """
      return [
          f"{lang}.{track}.test.gold", #{eng} {sentence}
          f"{lang}.{track}.dev", #{eng} {sentence}
      ]

  def get_raw_corpus(files: list) -> list:
      """Descarga y concatena los datos de los archivos tsv desde una URL base.

      Parameters:
      ----------
      files : list
          Lista de nombres de archivos (sin extensión) que se descargarán
          y concatenarán.

      Returns:
      -------
      list
          Una lista que contiene los contenidos descargados y concatenados
          de los archivos tsv.
      """
      result = []
      for file in files:
          print(f"Downloading {file}.tsv")
          r = requests.get(f"https://raw.githubusercontent.com/sigmorphon/2022SegmentationST/main/data/{file}.tsv")
          response_list = r.text.split("\n")
          result.extend(response_list[:-1]) #No entiendo que hace exactamente esta linea
      #Obtenemos una lista así:
      #[astronómica \t astronómico @@a \t 100]
      #[resignifiques \t resignificar @@es \t 100]
      #[importunamente \t importuno @@mente \t 010]
      #[conjeturaríamos \t conjeturar @@ría @@amos \t 100]
      return result
  
  

  #decidimos utilizar el idioma ingles
  corpus = get_raw_corpus(get_files("eng","sentence"))

  print("\n\n")
  print("Frases:\n")

  #observamos el corpus en crudo
  corpus

  #observamos el tamaño del corpus
  len(corpus)

  """
    Funcion que dado un corpues y un entero num_instancias, toma tal
    num_instancias de elementos aleatorios del corpus.

    Ademas, imprime la sentencia que toma.
  """
  def toma_n_instancias_random(corpus,num_instancias):
    lst = []
    while True:
      i = random.randint(0, len(corpus)-1)
      if len(lst) == num_instancias:
        break
      elem = corpus[i]
      if (elem not in lst) and (elem != '------\t------') and ("@@" in elem):
        print(corpus[i])
        lst.append(corpus[i])
    return lst

  #tomamos 10 al azar
  lst = toma_n_instancias_random(corpus,10)

  #Dividimos cada elemento en la phrase y su estructura, guardandolo en el mismo indice
  for i in range(len(lst)):
    lst[i] = lst[i].split('\t')

  #Aqui observamos el resultado
  lst

  """# ✅ Obtención de información morfologica (spacy)"""

  #Para esta practica, solo nos interesa la frase tal cual, asi solo guardaremos esta para cada elemento
  phrases = [l[0] for l in lst]
  phrases

  def extract_morphology(nlp,phrases):
    print("\n\n________ ___Obtención de información morfologica (spacy)___ ________\n")
    i=0
    for phrase in phrases:
      print("<<<Phrase",i+1,">>>")
      i+=1

      doc = nlp_en(phrase)
      for token in doc:
        print(token, token.morph.to_dict())
      print()

  nlp_en = spacy.load("en_core_web_sm")
  extract_morphology(nlp_en,phrases)

  """# ✅ Lemmatization (spacy)"""

  def extract_lemma(phrases, nlp):
    print("\n\n________  ___Lemmatization (spacy)___ ________\n")
    i = 0
    for phrase in phrases:
      doc = nlp(phrase)
      print("<<<Phrase",i+1,">>>")
      i+=1
      for token in doc:
          print(token,">>>",token.lemma_)
      print()

  nlp = spacy.load("en_core_web_sm")
  extract_lemma(phrases,nlp)

  """# ✅ Stemming (nltk)"""

  from nltk.stem import PorterStemmer
  stemmer = PorterStemmer() # Initialize the Porter stemmer

  def extract_stem(phrases):
    print("\n\n________ ___Stemming (nltk)___ ________\n")
    i = 0
    for phrase in phrases:
      print("<<<Phrase",i+1,">>>")
      i+=1
      phrase = phrase.split()

      # Stem each word
      stemmed_words = [stemmer.stem(word) for word in phrase]

      for p, st in zip(phrase, stemmed_words):
        print(p,">>>",st)
      print()

  extract_stem(phrases)