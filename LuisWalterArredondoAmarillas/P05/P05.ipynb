{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06e9488a-a237-4d7e-b474-360f189832d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: elotl in /home/xbmu/.local/lib/python3.8/site-packages (0.0.1.16)\n",
      "Requirement already satisfied: importlib-resources in /home/xbmu/.local/lib/python3.8/site-packages (from elotl) (6.1.0)\n",
      "Requirement already satisfied: future in /home/xbmu/.local/lib/python3.8/site-packages (from elotl) (0.18.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/xbmu/.local/lib/python3.8/site-packages (from importlib-resources->elotl) (3.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: subword-nmt in /home/xbmu/.local/lib/python3.8/site-packages (0.3.8)\n",
      "Requirement already satisfied: mock in /home/xbmu/.local/lib/python3.8/site-packages (from subword-nmt) (5.1.0)\n",
      "Requirement already satisfied: tqdm in /home/xbmu/.local/lib/python3.8/site-packages (from subword-nmt) (4.66.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /home/xbmu/.local/lib/python3.8/site-packages (3.7.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/xbmu/.local/lib/python3.8/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/xbmu/.local/lib/python3.8/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/xbmu/.local/lib/python3.8/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/xbmu/.local/lib/python3.8/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/xbmu/.local/lib/python3.8/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/xbmu/.local/lib/python3.8/site-packages (from spacy) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/xbmu/.local/lib/python3.8/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/xbmu/.local/lib/python3.8/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/xbmu/.local/lib/python3.8/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/xbmu/.local/lib/python3.8/site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/xbmu/.local/lib/python3.8/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/xbmu/.local/lib/python3.8/site-packages (from spacy) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/xbmu/.local/lib/python3.8/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/xbmu/.local/lib/python3.8/site-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/xbmu/.local/lib/python3.8/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/xbmu/.local/lib/python3.8/site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/xbmu/.local/lib/python3.8/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/xbmu/.local/lib/python3.8/site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/xbmu/.local/lib/python3.8/site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/xbmu/.local/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/xbmu/.local/lib/python3.8/site-packages (from spacy) (1.24.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/xbmu/.local/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /home/xbmu/.local/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/xbmu/.local/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/xbmu/.local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/xbmu/.local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/xbmu/.local/lib/python3.8/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/xbmu/.local/lib/python3.8/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/xbmu/.local/lib/python3.8/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.16.0,>=0.7.0 in /home/xbmu/.local/lib/python3.8/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.15.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/xbmu/.local/lib/python3.8/site-packages (from jinja2->spacy) (2.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cess_esp to /home/xbmu/nltk_data...\n",
      "[nltk_data]   Package cess_esp is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import math\n",
    "from collections import Counter\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import re\n",
    "!pip install elotl\n",
    "import elotl.corpus\n",
    "!pip install subword-nmt\n",
    "!pip install spacy\n",
    "import spacy \n",
    "spacy.load('en_core_web_sm')\n",
    "spacy.load('es_core_news_sm')\n",
    "\n",
    "nltk.download(\"cess_esp\")\n",
    "from nltk.corpus import cess_esp as cess\n",
    "def lemmatize(words: list, lang=\"en\") -> list:\n",
    "    model = \"en_core_web_sm\" if lang == \"en\" else \"es_core_news_sm\"\n",
    "    nlp = spacy.load(model)\n",
    "    nlp.max_length = 1500000\n",
    "    lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "    return [token.lemma_ for token in nlp(\" \".join(words))]\n",
    "\n",
    "\n",
    "axolotl = elotl.corpus.load(\"axolotl\")\n",
    "BIBLE_FILE_NAMES = {\"spa\": \"spa-x-bible-reinavaleracontemporanea\", \"eng\": \"eng-x-bible-kingjames\"}\n",
    "\n",
    "def get_bible_corpus(lang: str) -> str:\n",
    "    file_name = BIBLE_FILE_NAMES[lang]\n",
    "    r = requests.get(f\"https://raw.githubusercontent.com/ximenina/theturningpoint/main/Detailed/corpora/corpusPBC/{file_name}.txt.clean.txt\")\n",
    "    return r.text\n",
    "\n",
    "def write_plain_text_corpus(raw_text: str, file_name: str) -> None:\n",
    "    with open(f\"{file_name}.txt\", \"w\") as f:\n",
    "        f.write(raw_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb82426-39f9-4824-9f02-0fe39342cdf9",
   "metadata": {},
   "source": [
    "## Corpus Nahuatl (axolotl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "249174c3-7804-48ad-a515-5a695b93e65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|########################################| 300/300 [00:02<00:00, 143.38it/s]\n"
     ]
    }
   ],
   "source": [
    "train_rows_count = len(axolotl) - round(len(axolotl)*.3)\n",
    "axolotl_train = axolotl[:train_rows_count]\n",
    "axolotl_test = axolotl[train_rows_count:]\n",
    "axolotl_words_vanilla_train = [word for row in axolotl_train for word in row[1].lower().split()]\n",
    "write_plain_text_corpus(\" \".join(axolotl_words_vanilla_train), \"axolotl_plain_vanilla\")\n",
    "!subword-nmt learn-bpe -s 300 < axolotl_plain_vanilla.txt > axolotl_vanilla.model\n",
    "axolotl_test_words = [word for row in axolotl_test for word in row[1].lower().split()]\n",
    "axolotl_test_types = Counter(axolotl_test_words)\n",
    "axolotl_singletons = [singleton for singleton in axolotl_test_types.items() if singleton[1] == 1]\n",
    "write_plain_text_corpus(\" \".join(axolotl_test_words), \"axolotl_plain_test\")\n",
    "!subword-nmt apply-bpe -c axolotl_vanilla.model < axolotl_plain_test.txt > axolotl_vanilla_tokenized.txt\n",
    "with open(\"axolotl_vanilla_tokenized.txt\") as f:\n",
    "    axolotl_test_tokenized = f.read().split()\n",
    "axolotl_test_tokenized_types = Counter(axolotl_test_tokenized)\n",
    "axolotl_singletons_tokenized = [singleton for singleton in axolotl_test_tokenized_types.items() if singleton[1] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "913a22df-baa7-4dec-8b8b-5ec9864b4499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Axolotl Information\n",
      "Tokens: 86604\n",
      "Types (vanilla): 25714\n",
      "Types (native BPE): 451\n",
      "TTR (Vanilla): 0.2969146921620249\n",
      "TTR (BPE): 0.0017565178105453385\n",
      "Singletons: 19553\n",
      "Singletons (Tokenized): 16\n"
     ]
    }
   ],
   "source": [
    "print(\"Axolotl Information\")\n",
    "print(\"Tokens:\", len(axolotl_test_words))\n",
    "print(\"Types (vanilla):\", len(axolotl_test_types))\n",
    "print(\"Types (native BPE):\", len(axolotl_test_tokenized_types))\n",
    "print(\"TTR (Vanilla):\", len(axolotl_test_types)/len(axolotl_test_words))\n",
    "print(\"TTR (BPE):\", len(axolotl_test_tokenized_types)/len(axolotl_test_tokenized))\n",
    "print(\"Singletons:\", len(axolotl_singletons))\n",
    "print(\"Singletons (Tokenized):\", len(axolotl_singletons_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c467963-65b9-4244-a87b-5a9b3e37d116",
   "metadata": {},
   "source": [
    "## Corpus Biblia en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e84c769-d89a-4c83-a573-7c6d4e7c66a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|########################################| 300/300 [00:00<00:00, 396.91it/s]\n"
     ]
    }
   ],
   "source": [
    "cess_sents = cess.sents()\n",
    "cess_words = cess.words()\n",
    "cess_plain_text = \" \".join([\" \".join(sentence) for sentence in cess_sents])\n",
    "cess_plain_text = re.sub(r\"[-|_]\", \" \", cess_plain_text)\n",
    "with open(\"cess_plain.txt\", \"w\") as f:\n",
    "    f.write(cess_plain_text)\n",
    "!subword-nmt learn-bpe -s 300 < cess_plain.txt > cess.model\n",
    "spa_bible_plain_text = get_bible_corpus('spa')\n",
    "spa_bible_words = spa_bible_plain_text.replace(\"\\n\", \" \").split()\n",
    "spa_bible_types = Counter(spa_bible_words)\n",
    "spa_bible_lemmas_types = Counter(lemmatize(spa_bible_words, lang=\"es\"))\n",
    "write_plain_text_corpus(spa_bible_plain_text, \"spa-bible\")\n",
    "!subword-nmt apply-bpe -c cess.model < spa-bible.txt > spa_bible_tokenized.txt\n",
    "with open(\"spa_bible_tokenized.txt\", \"r\") as f:\n",
    "    tokenized_text = f.read()\n",
    "spa_bible_tokenized = tokenized_text.split()\n",
    "spa_bible_tokenized_types = Counter(spa_bible_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9a3efde-e503-4849-9d4f-46cdaef6787f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bible Spanish Information\n",
      "Tokens: 30073\n",
      "Types (vanilla): 3568\n",
      "Types (lemmatized) 2313\n",
      "Types (native BPE): 392\n",
      "TTR (Vanilla): 0.11864463139693412\n",
      "TTR (BPE): 0.006288904575498942\n"
     ]
    }
   ],
   "source": [
    "print(\"Bible Spanish Information\")\n",
    "print(\"Tokens:\", len(spa_bible_words))\n",
    "print(\"Types (vanilla):\", len(spa_bible_types))\n",
    "print(\"Types (lemmatized)\", len(spa_bible_lemmas_types))\n",
    "print(\"Types (native BPE):\", len(spa_bible_tokenized_types))\n",
    "print(\"TTR (Vanilla):\", len(spa_bible_types)/len(spa_bible_words))\n",
    "print(\"TTR (BPE):\", len(spa_bible_tokenized_types)/len(spa_bible_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f869fc9-0c3d-4b05-9bb3-72bd14e8855a",
   "metadata": {},
   "source": [
    "## Entropía de un texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370e8b0b-099c-4ef7-9e13-e63e7f7271fd",
   "metadata": {},
   "source": [
    "La entropía de un texto es una medida que nos permite evaluar cuán impredecible o caótico es un conjunto de datos textual. Se utiliza ampliamente en teoría de la información y procesamiento de lenguaje natural para comprender la complejidad de un texto y su contenido informativo. Esta métrica se basa en la probabilidad de ocurrencia de símbolos individuales en el texto, lo que nos permite cuantificar cuánta información o incertidumbre hay en el texto.\n",
    "\n",
    "La fórmula para calcular la entropía de un texto se presenta como:\n",
    "\n",
    "$H(X) = -\\sum_{i=1}^{n} p(x_i) \\log_2(p(x_i))$\n",
    "\n",
    "Donde $(H(X))$ es la entropía del texto, $(n)$ es el número de símbolos únicos en el texto, $(p(x_i))$ es la probabilidad de que el símbolo $(x_i)$ aparezca en el texto y $(log_2)$ es el logaritmo en base 2. Cuanto mayor sea la entropía, más impredecible es el texto, mientras que una entropía baja indica un texto más predecible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f2fc3c-f2a1-41db-97c7-42e02c8ad104",
   "metadata": {},
   "source": [
    "### Entropía Corpus biblia español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58f1d6b5-4835-4442-959b-b07325214aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_probabilities_spa_types = {word: count / len(spa_bible_words) for word, count in spa_bible_types.items()}\n",
    "entropy_spa_types = -sum(prob * math.log(prob, 2) for prob in word_probabilities_spa_types.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dccdcdc-b464-4f25-b7d7-54c106334e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999347"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#debe sumar 1\n",
    "total = sum(value for value in word_probabilities_spa_types.values())\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f552b7b8-111d-472a-b351-b1e4e4872491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.553905984312227"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_spa_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a1c845-a50f-40b4-a32a-c66004131d0d",
   "metadata": {},
   "source": [
    "### Entropía Corpus biblia español (tokenizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6e84e1d-8060-4ef8-860f-94ff302b782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_probabilities_spa_tokenized_types = {word: count / len(spa_bible_words) for word, count in spa_bible_tokenized_types.items()}\n",
    "entropy_spa_tokenized_types = -sum(prob * math.log(prob, 2) for prob in word_probabilities_spa_tokenized_types.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67db4a9c-c6fa-41cc-80f8-0535a6e890a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.54100128153741"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_spa_tokenized_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcebe457-5ab5-46bb-bd19-c65361d4aa26",
   "metadata": {},
   "source": [
    "### Entropía Corpus Axolotl Nahuatl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a7dc9e9-f552-4232-a7e0-d725f492278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_probabilities_nah_types = {word: count / len(axolotl_test_words) for word, count in axolotl_test_types.items()}\n",
    "entropy_nah_types = -sum(prob * math.log(prob, 2) for prob in word_probabilities_nah_types.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "162d8667-70dd-431f-8d80-00c7972e040a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000001217"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#debe sumar 1\n",
    "total = sum(value for value in word_probabilities_nah_types.values())\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b233cb11-9214-4614-9211-99d33d277855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.415959291241606"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_nah_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12679b3-7b91-4f61-8a24-0009e070a60c",
   "metadata": {},
   "source": [
    "### Entropía Corpus Axolotl Nahuatl (tokenizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c404f542-d29a-4ee7-82f6-2d1192df47be",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_probabilities_nah_tokenized_types = {word: count / len(axolotl_test_words) for word, count in axolotl_test_tokenized_types.items()}\n",
    "entropy_nah_tokenized_types = -sum(prob * math.log(prob, 2) for prob in word_probabilities_nah_tokenized_types.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69c7c70e-ee65-4f26-9eb9-058e0746d733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.66060699433508"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_nah_tokenized_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1632aabb-dbeb-402f-84ee-cb0f00a33a5e",
   "metadata": {},
   "source": [
    "## Preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe5e9bd-e61f-44cb-a2a0-0718e69fbf00",
   "metadata": {},
   "source": [
    "### ¿Aumento o disminuyó la entropia para los corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a0cf43-eb64-4ba6-b4d1-0311b396d932",
   "metadata": {},
   "source": [
    "En los dos corpus aumentó la entropía"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaaa13e-eab1-49ef-bf71-68a6cc87c0c8",
   "metadata": {},
   "source": [
    "### ¿Qué significa que la entropia aumente o disminuya en un texto?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432e2eaa-2818-4992-9486-4bb84fd48f92",
   "metadata": {},
   "source": [
    "Cuando aumenta la entropía, significa que dada una palabra es más difícil predecir la siguiente, es decir que el texto es más aleatorio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cd0fb0-9fa9-446d-9053-f9bafa6d54e5",
   "metadata": {},
   "source": [
    "### ¿Como influye la tokenizacion en la entropía de un texto?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b706d434-ef19-468f-b45a-980f30f0dc2e",
   "metadata": {},
   "source": [
    "Se pierde información y hace más difícil predecir un texto, sin embargo se reduce la cantidad de palabras de una sola ocurrencia en el corpus (singletons) lo cual hace más fácil el análisis del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc2b425-f0a9-4bdf-9615-4773fbce02a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
