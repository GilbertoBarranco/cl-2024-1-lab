# -*- coding: utf-8 -*-
"""Práctica 5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MFOwa6z5BuyWklCTPp9EH4djInhLEZDk
"""

import requests


def get_bible_corpus(lang: str) -> str:
    """
    Obtain the Bible file.
    """
    file_name = BIBLE_FILE_NAMES[lang]
    r = requests.get(
        f"https://raw.githubusercontent.com/ximenina/theturningpoint/main/Detailed/corpora/corpusPBC/{file_name}.txt.clean.txt"
    )
    return r.text


def write_plain_text_corpus(raw_text: str, file_name: str) -> None:
    """
    Creates the {file_name}.txt file and writes raw_text into it.
    """
    with open(f"{file_name}.txt", "w") as f:
        f.write(raw_text)


import math


def Sentropy(tokens: list) -> float:
    """
    Returns Shanon's entropy from a list.
    """
    types = dict.fromkeys(tokens)

    # Calculate p_k's
    pk_vect = [float(tokens.count(t)) / len(tokens) for t in types]

    # Calculate Shannon's entropy
    H = -sum([pk * math.log(pk, 2) for pk in pk_vect])
    return H


import nltk

# Downloading the Spanish corpus that we are going to work with.
nltk.download("cess_esp")
from nltk.corpus import cess_esp as cess

import re

cess_sents = cess.sents()

# Converts cess_sents into a cess_plain_text string.
cess_plain_text = " ".join([" ".join(sentence) for sentence in cess_sents])

# Replace _ and -  in cess_plain_text for blank spaces.
cess_plain_text = re.sub(r"[-|_]", " ", cess_plain_text)

"""The following is training a BPE model for Spanish"""

# Creating cess_plain.txt and writing cess_plain_text into it.
write_plain_text_corpus(cess_plain_text, "cess_plain")

"""HERE, WE TRAIN cess.model"""

"""Using cess.model to tokenize Bible (Spanish translation)"""

BIBLE_FILE_NAMES = {
    "spa": "spa-x-bible-reinavaleracontemporanea",
    "eng": "eng-x-bible-kingjames",
}
BIBLE_FILE_NAMES["spa"]

spa_bible_plain_text = get_bible_corpus("spa")
spa_bible_plain_text = spa_bible_plain_text.replace("\n", " ")

spa_bible_words = spa_bible_plain_text.split()

# The following list will be required to calculate entropy without tokenization.
spa_bible_chars = [char for char in "".join(spa_bible_words)]

# Creating spa_bible.txt and writing spa_bible_plain_txt into it.
write_plain_text_corpus(spa_bible_plain_text, "spa-bible")

"""HERE IS TOKENIZATION OF spa_bible_plain.txt OCCOURS AND GENERATING spa_bible_tokenized.txt"""

with open("spa_bible_tokenized.txt", "r") as f:
    tokenized_text = f.read()

# The following list will be required to calculate entropy with tokenization.
spa_bible_tokenized = tokenized_text.split()

"""The following is training a BPE model for Náhuatl"""

import elotl.corpus

axolotl = elotl.corpus.load("axolotl")

# Calculating the number of rows that we will use to train axolotl.vanilla.model
train_rows_count = len(axolotl) - round(len(axolotl) * 0.30)

axolotl_train = axolotl[:train_rows_count]
axolotl_test = axolotl[train_rows_count:]

# Obtaining náhuatl words from axolotl_train
axolotl_words_vanilla_train = [
    word for row in axolotl_train for word in row[1].lower().split()
]

# Creating axolotl_plain_vanilla.txt and writing " ".join(axolotl_words_vanilla_train) into it.
write_plain_text_corpus(" ".join(axolotl_words_vanilla_train), "axolotl_plain_vanilla")

"""HERE, WE TRAIN axolotl_vanilla.model USING axolotl_plain_vanilla.txt"""

# Obtaining the náhuatl words which weren't used in training step.
axolotl_test_words = [word for row in axolotl_test for word in row[1].lower().split()]

# The following will be useful to calculate entropy without tokenization.
axolotl_chars = [char for char in "".join(axolotl_test_words)]

# Creating axolotl_plain.txt and writing " ".join(axolotl_test_words) into it.
write_plain_text_corpus(" ".join(axolotl_test_words), "axolotl_plain")

"""WE TOKENIZE axolotl_plain.txt and GENERATES axolotl_vanilla_tokenized.txt"""

with open("axolotl_vanilla_tokenized.txt") as f:
    # The following will be required to calculate entropy with tokenization.
    axolotl_tokenized = f.read().split()

"""Calculating Shannon's entropy belonging to Bible (Spanish translation) and Axolotl (náhuatl-test words)"""

print("Calculamos la entropía de Shannon de los corpus anteriores.\n")
print("Entropía de la Biblia en español (sin tokenizar): ", Sentropy(spa_bible_chars))
print("Entropía de la Biblia en español (tokenizada): ", Sentropy(spa_bible_tokenized))
print("Entropía de Axolotl (sin tokenizar): ", Sentropy(axolotl_chars))
print("Entropía de Axolotl (tokenizado): ", Sentropy(axolotl_tokenized))
