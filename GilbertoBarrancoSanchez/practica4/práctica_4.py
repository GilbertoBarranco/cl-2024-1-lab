# -*- coding: utf-8 -*-
"""Práctica 4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z1Sa3F6IO5xc3xBDzJ6npd97lwY0DZBh
"""

# Libraries
from collections import Counter
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = [15, 6]
from re import sub
import numpy as np
import nltk

from elotl import corpus as elotl_corpus

def get_frequencies(vocabulary: Counter, n: int) -> list:
    """
    Return frequencies of the words in a vocabulary.
    """
    return [_[1] for _ in vocabulary.most_common(n)]

def plot_frequencies(frequencies: list, title="Freq of words"):
    x = list(range(1, len(frequencies)+1))
    plt.plot(x, frequencies, "-v")
    plt.xlabel("Freq rank (r)")
    plt.ylabel("Freq (f)")
    plt.title(title)

def extract_words_from_sentence(sentence: str) -> list:
    return sub(r'[^\w\s\']', ' ', sentence).lower().split()

def preprocess_corpus(corpus):
    # Obtains the sentence in L1
    # delete punctuation marks
    # obtains the list of words
    word_list_l1 = []
    word_list_l2 = []
    for row in corpus:
        word_list_l1.extend(extract_words_from_sentence(row[0]))
    # Obtains the sentence in L2
    # delete punctuation marks
    # obtains the list of word
        word_list_l2.extend(extract_words_from_sentence(row[1]))
    return word_list_l1, word_list_l2

def get_words_and_tags(corpus):
  """Return two lists:
      -List1: list of POS tags from a corpora.
      -List2: list of words form a corpora.
      List1 and List2 are corresponded one to one.
  """
  POS_corpus = [tag for word, tag in corpus]
  words_corpus = [word for word, tag in corpus]
  return words_corpus, POS_corpus

import requests

def get_tags_map():
    tags_raw = requests.get("https://gist.githubusercontent.com/vitojph/39c52c709a9aff2d1d24588aba7f8155/raw/af2d83bc4c2a7e2e6dbb01bd0a10a23a3a21a551/universal_tagset-ES.map").text.split("\n")
    tags_map = {line.split("\t")[0].lower(): line.split("\t")[1] for line in tags_raw}
    return tags_map

def map_tag(tag: str, tags_map=get_tags_map()) -> str:
    return tags_map.get(tag.lower(), "N/F")

nltk.download('brown')
nltk.download('universal_tagset')
english_corpus = nltk.corpus.brown.tagged_words(tagset='universal')
from nltk.corpus import cess_esp

# Load the Spanish POS tagged corpus
nltk.download("cess_esp")
spanish_corpus = cess_esp.tagged_sents()

#Tomamos una parte del corpus.
english_corpus = english_corpus[:150000]
# Inicializar una lista vacía para contener los elementos
elements = []

# Usar un bucle for para recorrer las sublistas y usar extend() para agregar sus elementos
for sentence in spanish_corpus:
    elements.extend(sentence)

spanish_corpus = elements

eng_words, POS_eng = get_words_and_tags(english_corpus)
spa_words, POS_spa = get_words_and_tags(spanish_corpus)

#Obtenemos los tags correspondientes a cada código.
POS_spa = [map_tag(tag) for tag in POS_spa]

POS_eng_vocabulary = Counter(POS_eng)
POS_spa_vocabulary = Counter(POS_spa)

print("Obtengamos las frecuencias de las etiquetas POS para Inglés y Español.")

most_common_count = 11
POS_spa_freqs = get_frequencies(POS_spa_vocabulary, most_common_count)
plot_frequencies(POS_spa_freqs, f"Frequencies for POS Spanish {most_common_count} most common")

POS_eng_freqs = get_frequencies(POS_eng_vocabulary, most_common_count)
plot_frequencies(POS_eng_freqs, f"Frequencies for POS English {most_common_count} most common")

print("Comprobaremos si se cumple la Ley de Zipf")

a = 1.8
N = 1000
zipf_distribution = np.random.zipf(a, N)
zipf_numbers_freqs = get_frequencies(Counter(zipf_distribution), 11)

x = list(range(1, 12))
plt.figure()
plt.loglog(x, zipf_numbers_freqs, label="Zipf generated")
plt.loglog(x, POS_spa_freqs, "-v", label="Spanish")
plt.loglog(x, POS_eng_freqs, "-v", label="English")
plt.legend()
plt.show()

print("Por lo que se puede observar en la gráfica anterior, no parece que \nlas frecuencias de las etiquetas POS satisfagan la Ley de Zipf. Podríamos considerar\notros lenguajes.")

def get_characters(words:list):
  """
    Returns the list of characters in a list of words (avoid punctuation marks).
  """
  join1 = "".join(words)
  words = extract_words_from_sentence(join1)
  join2 = "".join(words)
  characters = list(join2)
  return characters



print("Ahora obtengamos las frecuencias asociadas a los caracteres de los corpus para los siguientes lenguajes.")
print("Lenguajes: Náhuatl, Otomí, Español")

from elotl import corpus as elotl_corpus

axolotl = elotl_corpus.load("axolotl")
tsunkua = elotl_corpus.load("tsunkua")

spanish_words_na, nah_words = preprocess_corpus(axolotl)
spanish_words_oto, otom_words = preprocess_corpus(tsunkua)

nah_chars = get_characters(nah_words)
otom_chars = get_characters(otom_words)
spa_chars = get_characters(spa_words)

nah_chars_voc = Counter(nah_chars)
otom_chars_voc = Counter(otom_chars)
spa_chars_voc = Counter(spa_chars)

most_common_count = 45
nahchar_freqs = get_frequencies(nah_chars_voc, most_common_count)
plot_frequencies(nahchar_freqs, f"Frequencies for chars in Náhuatl {most_common_count} most common")

otomchar_freqs = get_frequencies(otom_chars_voc, most_common_count)
plot_frequencies(otomchar_freqs, f"Frequencies for chars in Otomí {most_common_count} most common")

spachar_freqs = get_frequencies(spa_chars_voc, most_common_count)
plot_frequencies(spachar_freqs, f"Frequencies for chars in Spanish {most_common_count} most common")

print("Comprobemos si se satisface la Ley de Zipf para las frecuencias de estos datos")

a = 1.18
N = 1000
zipf_distribution = np.random.zipf(a, N)
zipf_numbers_freqs = get_frequencies(Counter(zipf_distribution), 45)

x = list(range(1, 46))
plt.figure()
plt.loglog(x, zipf_numbers_freqs, label="Zipf generated")
plt.loglog(x, nahchar_freqs, "-v", label="Náhuatl")
plt.loglog(x, otomchar_freqs, "-v", label="Otomí")
plt.loglog(x, spachar_freqs, "-v", label="Spanish")
plt.legend()
plt.show()

print("Por lo que muestra el gráfico anterior, tampoco parece que los caracteres que forman las palabras \nde cada lengua, cumplan con la Ley de Zipf")

print("Ahora encontremos las frecuencias para n-gramas, correspondientes a los lenguajes anteriores.")
print("Consideremos n = 2.")

from nltk.util import ngrams

def get_ngrams(words: list, n = 2):
  """
    Return 2-grams from a list of words (avoid punctuation marks).
  """
  new_words = []
  for word in words:
        new_words.extend(''.join(extract_words_from_sentence(word)))
  n_grams = ["".join(ngram) for ngram in list(ngrams(new_words, n))]
  return n_grams

nah_ngrams = get_ngrams(nah_words)
otom_ngrams = get_ngrams(otom_words)
spa_ngrams = get_ngrams(spa_words)

nah_ngrams_voc = Counter(nah_ngrams)
otom_ngrams_voc = Counter(otom_ngrams)
spa_ngrams_voc = Counter(spa_ngrams)

most_common_count = 300
nah_ngrams_freqs = get_frequencies(nah_ngrams_voc, most_common_count)
plot_frequencies(nah_ngrams_freqs, f"Frequencies for ngrams in Náhuatl {most_common_count} most common")

otom_ngrams_freqs = get_frequencies(otom_ngrams_voc, most_common_count)
plot_frequencies(otom_ngrams_freqs, f"Frequencies for ngrams in Otomí {most_common_count} most common")

spa_ngrams_freqs = get_frequencies(spa_ngrams_voc, most_common_count)
plot_frequencies(spa_ngrams_freqs, f"Frequencies for ngrams in Spanish {most_common_count} most common")

a = 1.4
N = 100000
zipf_distribution = np.random.zipf(a, N)
zipf_numbers_freqs = get_frequencies(Counter(zipf_distribution), 300)

x = list(range(1, 301))
plt.figure()
plt.loglog(x, zipf_numbers_freqs, label="Zipf generated")
plt.loglog(x, nah_ngrams_freqs, "-v", label="Náhuatl")
plt.loglog(x, otom_ngrams_freqs, "-v", label="Otomí")
plt.loglog(x, spa_ngrams_freqs, "-v", label="Spanish")
plt.legend()
plt.show()

print("A pesar de que los 2-gramas con menos frecuencias no parecen tener un comportamiento lineal, \naquellos con más frecuencia sí parecen tenerlo.")
print("Me atrevería a decir que las frecuencias de los 2-gramas sí satisfacen la Ley de Zipf")

print("Ahora veamos si las stopwords en alguna paquetería coinciden con las palabras más comunes que obtuvimos")

nltk.download('stopwords')
spa_stopwords = nltk.corpus.stopwords.words('spanish')
eng_stopwords = nltk.corpus.stopwords.words('english')

print("Número de stopwords para Español:", len(spa_stopwords))
print("Número de stopwords para Inlgés", len(eng_stopwords))

print("Para ver si las stopwords coinciden, se muestran las siguientes nubes de palabras\n")
print("Ya no me dio tiempo de hacer las nubes :(")
