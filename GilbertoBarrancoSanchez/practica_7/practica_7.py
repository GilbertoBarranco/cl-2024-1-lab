# -*- coding: utf-8 -*-
"""practica_7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dkEttoJ_Z7zYjZrFn413KXd0Zz_dXTfD
"""

import requests

def get_file(url, file_name):
  """
  Given an URL, returns the file contained in the link.
  """

  response = requests.get(url)

  # Check if the request was successful (state code 200).
  if response.status_code == 200:
      # Save the file in a local file.
      with open(f"{file_name}", "w", encoding="utf-8") as file:
          file.write(response.text)
          return file
  else:
      print(f"Error al descargar el archivo. CÃ³digo de estado: {response.status_code}")

#Obtain corpus.

url = "https://www.gutenberg.org/cache/epub/2000/pg2000.txt"
file_txt = get_file(url, "quijote.txt")

with open("quijote.txt", "r", encoding = "utf8") as archivo:
    lines = archivo.readlines()

#The book starts in line 26 and ends in line 37697.
lines = lines[26:37697]

#Deleting isolated line breaks.
lines = [sentence for sentence in lines if sentence != "\n" and sentence != "  \n"]

import re
"""
Takes a list of strings and return a list in which each string turns into a list
that contains the string words.
"""
def preprocess_corpus(corpus: list[str]) -> list:
    clean_corpus = []
    for sent in corpus:
        clean_corpus.append([word.lower() for word in sent.split() if re.match("^(?![0-9]+)[\w\s]+", word)])
    return clean_corpus

#Preprocess lines.
corpus = preprocess_corpus(lines)

#Getting a sample of corpus to avoid our computer to die.
corpus = corpus[:200]

# Split corpus into training and testing sets.

from sklearn.model_selection import train_test_split

corpus_train, corpus_test = train_test_split(corpus, test_size=0.3)

len(corpus_train) + len(corpus_test) == len(corpus)

print("Train len:", len(corpus_train), "\nTest len:", len(corpus_test))

from collections import defaultdict, Counter

def vocabulary_factory():
    """Function that create a vocabulary

    Default method when a key is not in the dictionary changed to be the
    current lenght of the dictionary to provide a unique index for each
    new key.

    Example:
    >> vocab['test']
    0
    >> vocab['other']
    1
    >> vocab['test']
    0
    """
    vocab = defaultdict()
    vocab.default_factory = lambda: len(vocab)
    return vocab

def word_to_index(corpus: list[list[str]], vocab: defaultdict) -> list[int]:
    """Function that maps each word in a corpus to a unique index"""
    for sent in corpus:
        yield [vocab[word] for word in sent]

vocab = vocabulary_factory()

indexed_sents = list(word_to_index(corpus_train, vocab))

#Adding BOS and EOS tags to each sentence of corpus.

BOS = ""
EOS = ""

BOS_IDX = max(vocab.values())+2
EOS_IDX = max(vocab.values())+1

vocab[BOS] = BOS_IDX
vocab[EOS] = EOS_IDX

indexed_corpus_train = [[BOS_IDX] + sent + [EOS_IDX] for sent in indexed_sents]

def get_index_to_word(vocab: defaultdict) -> dict:
    """Map indices as keys and words as values from a vocabulary"""
    return {index: word for word, index in vocab.items()}

vocab_words = get_index_to_word(vocab)

#Having a preprocessed corpus, it starts to estimate the model.
import numpy as np
from itertools import chain

def get_n_grams(indexed_sents: list[list[str]], n=2) -> chain:
    return chain(*[zip(*[sent[i:] for i in range(n)]) for sent in indexed_sents])

def get_model(sents: list[list[str]], vocabulary: defaultdict, n: int=2, l: float=1.0) -> tuple:

    # Get n_grams
    n_grams = get_n_grams(sents, n)

    # Get n_grams frequencies
    freq_n_grams = Counter(n_grams)

    # Get vocabulary length (without BOS/EOS)
    N = len(vocabulary) - 1
    # Calculate tensor dimentions for transition probabilities
    # For columns (conditional word) we consider the EOS element so we add 1
    dim = (N,)*(n-1) + (N+1,)

    # Transition tensor
    A = np.zeros(dim)
    # Initial Probabilities
    Pi = np.zeros(N)

    for n_gram, frec in freq_n_grams.items():
      # Fill the tensor with frequencies
      if n_gram[0] != BOS_IDX:
          A[n_gram] = frec
      # Getting initial frequencies
      elif n_gram[0] == BOS_IDX and n_gram[1] != EOS_IDX:
          Pi[n_gram[1]] = frec

    # Calculating probabilities from frequencies
    # We consider the parameter `l` for Lidstone Smoothing
    for h, b in enumerate(A):
      A[h] = ((b+l).T/(b+l).sum(n-2)).T

    # Calculating initial probabilities
    Pi = (Pi+l)/(Pi+l).sum(0)

    # We get our model
    return A, Pi

# %%time
bigram_model = get_model(indexed_corpus_train, vocab, n=2, l=1)

A_bigram = bigram_model[0]
print("Tensor dimention (bigram model)", A_bigram.shape)
print("Suma de probabilidades")
print(A_bigram.sum(1))

# #Estimating the model (Laplace Smoothing).
# %%time
trigram_model = get_model(indexed_corpus_train, vocab, n=3, l=1)

A_trigram = trigram_model[0]
print("Tensor dimention (trigram model)", A_trigram.shape)
print("Suma de probabilidades")
print(A_trigram.sum(1))

#To obtain the probability of a sentence.
def get_sent_probability(sentence: str, vocab: defaultdict, model: tuple) -> float:
  """
  Gets a sentence, a dictionary that assigns a word for each index in it and a matrix
  with probabilities. Returns a log probability.
  """
  A, Pi = model
  # Getting the n from n-grams
  n = len(A.shape)
  indexed_sentence = [vocab[word] for word in sentence.split()]
  first_indexed_word = indexed_sentence[0]
  # Getting initial probability
  try:
    probability = np.log(Pi[first_indexed_word])
  except:
    probability = 0.0

  # Getting n-grams of the sentence
  n_grams = get_n_grams([indexed_sentence], n)
  for n_gram in n_grams:
    try:
      probability += np.log(A[n_gram])
    except:
      probability += 0.0

  return probability

#Calculating perplexity.

def perplexity(sentence: str, model:tuple):
  """
  Gets a sentence for testing and a n-gram model and returns the model's perplexity.
  """
  num_words = len(sentence.split())
  return np.power(np.exp(get_sent_probability(sentence, vocab, model)), -1/num_words)

#Sentence to calculate perplexity.
test_sentences = [" ".join(corpus_test[i]) for i in range(len(corpus_test))]
test_sentence = " ".join(test_sentences)

#Bigram model perplexity.
print("La perplejidad del modelo de bigramas es: ", perplexity(test_sentence, bigram_model))

#Trigram model perplexity.
print("La perplejidad del modelo de trigramas es: ", perplexity(test_sentence, trigram_model))
