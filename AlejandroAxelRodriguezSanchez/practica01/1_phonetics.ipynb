{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Práctica 1: Buscador equivalencias fonéticas en un corpus\n",
        "\n",
        "Elaborado por: Alejandro Axel Rodríguez Sánchez  \n",
        "Correo: [ahexo@ciencias.unam.mx](mailto:ahexo@ciencias.unam.mx)  \n",
        "Github: [@Ahexo](https://github.com/Ahexo/)  \n",
        "Número de Cuenta: 315247697  \n",
        "Institución: Facultad de Ciencias UNAM  \n",
        "Asignatura: Lingüística computacional  \n",
        "Grupo: 7014  \n",
        "Semestre: 2024-1  \n",
        "\n",
        "---\n",
        "\n",
        "## Especificación de la práctica\n",
        "\n",
        "1. Agregar un nuevo modo de búsqueda donde se extienda el comportamiento básico del buscador para ahora buscar por frases.\n",
        "\n",
        "```\n",
        "[es_MX]>>> Hola que hace\n",
        " /ola/ /ke/ /ase/\n",
        "```\n",
        "\n",
        "2. Optimizar el código para agregar los datasets en un \"cache\" a demanda y no descargar todo el corpus de un trancazo. Esto quiere decir que al inicio de la ejecución no habrá ningun dataset descargado. Mientras la usuaria vaya seleccionado idiomas los irá agregando a un cache local (puede ser persistente o en memoria):\n",
        "\n",
        "```\n",
        "lang>> es_MX\n",
        "Corpus no encontrado. Descargando ...\n",
        "[es_MX]>>\n",
        "...\n",
        "lang>> en_US\n",
        "Corpus no encontrado. Descargando ...\n",
        "[en_US]>>\n",
        "...\n",
        "lang>> es_MX\n",
        "[es_MX]>>\n",
        "...\n",
        "```\n",
        "\n",
        "3. (Extra) Mejorar la solución al escenario cuando no se encuentran las palabras en el dataset mostrando palabras similares. Ejemplo:\n",
        "\n",
        "```\n",
        "[es_MX]>> pero\n",
        "No se encontro <<pero>> en el dataset. Palabras aproximadas:\n",
        "perro /pero/\n",
        "perno /peɾno/\n",
        "[es_MX]>>\n",
        "```\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "cVjUIDH621Yg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obteniendo el corpus"
      ],
      "metadata": {
        "id": "QW5PpzLXZR5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lang_codes = {\n",
        "  \"ar\": \"Arabic (Modern Standard)\",\n",
        "  \"de\": \"German\",\n",
        "  \"en_UK\": \"English (Received Pronunciation)\",\n",
        "  \"en_US\": \"English (General American)\",\n",
        "  \"eo\": \"Esperanto\",\n",
        "  \"es_ES\": \"Spanish (Spain)\",\n",
        "  \"es_MX\": \"Spanish (Mexico)\",\n",
        "  \"fa\": \"Persian\",\n",
        "  \"fi\": \"Finnish\",\n",
        "  \"fr_FR\": \"French (France)\",\n",
        "  \"fr_QC\": \"French (Québec)\",\n",
        "  \"is\": \"Icelandic\",\n",
        "  \"ja\": \"Japanese\",\n",
        "  \"jam\": \"Jamaican Creole\",\n",
        "  \"km\": \"Khmer\",\n",
        "  \"ko\": \"Korean\",\n",
        "  \"ma\": \"Malay (Malaysian and Indonesian)\",\n",
        "  \"nb\": \"Norwegian Bokmål\",\n",
        "  \"nl\": \"Dutch\",\n",
        "  \"or\": \"Odia\",\n",
        "  \"ro\": \"Romanian\",\n",
        "  \"sv\": \"Swedish\",\n",
        "  \"sw\": \"Swahili\",\n",
        "  \"tts\": \"Isan\",\n",
        "  \"vi_C\": \"Vietnamese (Central)\",\n",
        "  \"vi_N\": \"Vietnamese (Northern)\",\n",
        "  \"vi_S\": \"Vietnamese (Southern)\",\n",
        "  \"yue\": \"Cantonese\",\n",
        "  \"zh\": \"Mandarin\"\n",
        "}\n",
        "iso_lang_codes = list(lang_codes.keys())"
      ],
      "metadata": {
        "id": "YSRb9cx5jM8d"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definiendo funciones elementales para operar los datasets y su contenido\n",
        "Estas vienen copiadas de la especificación de la práctica, con ligeras modificaciones."
      ],
      "metadata": {
        "id": "mhz50Es-r3uA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def response_to_dict(ipa_list: list) -> dict:\n",
        "    \"\"\"Parse to dict the list of word-IPA\n",
        "\n",
        "    Each element of text hae the format:\n",
        "    [WORD][TAB][IPA]\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ipa_list: list\n",
        "        List with each row of ipa-dict raw dataset file\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict:\n",
        "        A dictionary with the word as key and the phonetic\n",
        "        representation as value\n",
        "    \"\"\"\n",
        "    result = {}\n",
        "    for item in ipa_list:\n",
        "        item_list = item.split(\"\\t\")\n",
        "        result[item_list[0]] = item_list[1]\n",
        "    return result\n",
        "\n",
        "import requests as r\n",
        "\n",
        "def get_ipa_dict(iso_lang: str) -> dict:\n",
        "    \"\"\"Get ipa-dict file from Github\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    iso_lang:\n",
        "        Language as iso code\n",
        "\n",
        "    Results:\n",
        "    --------\n",
        "    dict:\n",
        "        Dictionary with words as keys and phonetic representation\n",
        "        as values for a given lang code\n",
        "    \"\"\"\n",
        "    response = r.get(f\"https://raw.githubusercontent.com/open-dict-data/ipa-dict/master/data/{iso_lang}.txt\")\n",
        "    raw_data = response.text.split(\"\\n\")\n",
        "    return response_to_dict(raw_data[:-1])\n",
        "\n",
        "def get_ipa_transcriptions(word: str, dataset: dict) -> list[str]:\n",
        "    \"\"\"Search for word in a given dataset of IPA phonetics\n",
        "\n",
        "    Given a word this function return the IPA transcriptions\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    word: str\n",
        "        A word to search in the dataset\n",
        "    dataset: dict\n",
        "        A dataset for a given language code\n",
        "    Returns\n",
        "    -------\n",
        "    list:\n",
        "        A list with all the found IPA transcriptions of the input word.\n",
        "    \"\"\"\n",
        "    return dataset.get(word.lower(), \"NOT FOUND\").split(\", \")\n",
        "\n",
        "# El dataset comenzará vacío y se descargaran los corpus sobre demanda.\n",
        "dataset = {}\n"
      ],
      "metadata": {
        "id": "VwLvvuk5bj94"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definiendo el ciclo de ejecución"
      ],
      "metadata": {
        "id": "DMjlkx3A520E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Representación fonética de palabras\")\n",
        "print(f\"Lenguas disponibles: {(iso_lang_codes)}\")\n",
        "print(\"Deja la casilla en blanco y presiona enter para salir.\")\n",
        "\n",
        "lang = \"none\"\n",
        "\n",
        "while lang:\n",
        "  lang = input(\"lang>> \")\n",
        "  if lang in iso_lang_codes:\n",
        "\n",
        "    if lang not in dataset:\n",
        "      print(\"Corpus no encontrado. Descargando...\")\n",
        "      dataset.update({lang : get_ipa_dict(lang)})\n",
        "\n",
        "    sub_dataset = dataset[lang]\n",
        "\n",
        "    query = \"none\"\n",
        "    while query:\n",
        "      query = input(f\"[{lang}]>> \")\n",
        "\n",
        "      resultado_final = []\n",
        "      for palabra in query.split():\n",
        "        resultado = get_ipa_transcriptions(palabra, sub_dataset)\n",
        "\n",
        "        if resultado[0] != \"NOT FOUND\":\n",
        "          resultado_final += resultado\n",
        "        else:\n",
        "          resultado_final += \"/?/\"\n",
        "\n",
        "        resultado_final += [\" \"]\n",
        "\n",
        "      print(\"\".join(resultado_final))\n",
        "\n",
        "  elif lang:\n",
        "    print(\"Ese lenguaje no está disponible, intenta con otro:\")\n",
        "\n",
        "print(\"Adios 👋🏼\")\n"
      ],
      "metadata": {
        "id": "UXngrQb056b-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}