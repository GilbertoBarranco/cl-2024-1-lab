{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Practica 7: Modelos del lenguaje"
      ],
      "metadata": {
        "id": "9QUBm9_cSutX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objetivos\n",
        "\n",
        "- Crear modelos del lenguaje a partir de un corpus en inglés\n",
        "    - Modelo de bigramas\n",
        "    - Modelo de trigramas"
      ],
      "metadata": {
        "id": "B3yV4Myx5YRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Un modelo del lenguaje es un modelo estadístico que asigna probabilidades a cadenas dentro de un lenguaje - Jurafsky, 2000\n",
        "\n",
        "$$ \\mu = (\\Sigma, A, \\Pi)$$\n",
        "\n",
        "Donde:\n",
        "- $\\mu$ es el modelo del lenguaje\n",
        "- $\\Sigma$ es el vocabulario\n",
        "- $A$ es el tensor que guarda las probabilidades de trancisiones\n",
        "- $\\Pi$ guarda las probabilidades iniciales"
      ],
      "metadata": {
        "id": "nNyww-_MsZb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Este modelo busca estimar la probabilidad de una secuencia de tokens\n",
        "- Pueden ser palabras, caracteres o tokens\n",
        "- Se pueden considerar varios escenarios para la creación de estos modelos"
      ],
      "metadata": {
        "id": "YQV3jCdc689o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aplicaciones"
      ],
      "metadata": {
        "id": "_QdN5z6A5lsx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Traducción automática\n",
        "- Completado de texto\n",
        "- Generación de texto"
      ],
      "metadata": {
        "id": "DA3h5XD_5nhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## De los bigramas a los n-gramas"
      ],
      "metadata": {
        "id": "grLl2l7s56gx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Para bigramas tenemos la propiedad de Markov\n",
        "- Para $n > 2$ las palabras dependen de mas elementos\n",
        "    - Trigramas\n",
        "    - 4-gramas\n",
        "- En general para un modelo de n-gramas se toman en cuenta $n-1$ elementos"
      ],
      "metadata": {
        "id": "0sFQj5FyoW0A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obteniendo y preprocesando el texto"
      ],
      "metadata": {
        "id": "IWM8ppz0DGNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a trabajar con el corpus gutenberg disponible en el paquete NLTK"
      ],
      "metadata": {
        "id": "NVIKdyUN646x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "m4aP2awtEIS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAOp_8QVMf4y"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import gutenberg\n",
        "\n",
        "gutenberg.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gutenberg.sents(fileids=\"bible-kjv.txt\")[:3]"
      ],
      "metadata": {
        "id": "ephrCTgEETEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El preprocesamiento consistira en eliminar signos de puntuación y dejar todas las palabras en minúsculas\n",
        "- Cabe señalar que, dependiendo de la aplicación puede que sea necesario mantener los signos de puntuación como elementos del vocabulario\n",
        "- Para simplificar en la práctica no se considerarán"
      ],
      "metadata": {
        "id": "chIwKqOKF2p1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def preprocess_corpus(corpus: list[list[str]]) -> list:\n",
        "    clean_corpus = []\n",
        "    for sent in corpus:\n",
        "        clean_corpus.append([word.lower() for word in sent if re.match(\"^(?![0-9]+$)[\\w\\s]+$\", word)])\n",
        "    return clean_corpus\n",
        "\n",
        "corpus = preprocess_corpus(gutenberg.sents(fileids=\"bible-kjv.txt\"))"
      ],
      "metadata": {
        "id": "xbpAZJ_PFdfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus)"
      ],
      "metadata": {
        "id": "goDw99Si4DlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = corpus[:500]"
      ],
      "metadata": {
        "id": "NI5E4aS_uW8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[-10:]"
      ],
      "metadata": {
        "id": "KleDUgXg7i8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a partir el corpus en dos secciones. Una para train con la que entrenaremos el modelo y otra para probar el modelo"
      ],
      "metadata": {
        "id": "0Q6rNEY9ylvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "corpus_train, corpus_test = train_test_split(corpus, test_size=0.3)\n",
        "\n",
        "len(corpus_train) + len(corpus_test) == len(corpus)"
      ],
      "metadata": {
        "id": "DjvpPPufy1_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train len:\", len(corpus_train), \"test len:\", len(corpus_test))"
      ],
      "metadata": {
        "id": "tBwSyjvlvhDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nuestro modelo del lenguaje requiere que pasemos nuestras palabras a indices numericos. Utilizaremos enteros para estimar el modelo.\n",
        "Crearemos dos diccionarios:\n",
        "    1. el primero tomara la palabra y lo convertira a indice (Para acceder a las probabilidades del modelo)\n",
        "    2. El segundo tomará los indices y los convertira de vuelta a palabras (Nos ayudará a recuperar las palabras a partir de los índices del modelo)"
      ],
      "metadata": {
        "id": "RZ3Alqu-zQUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "\n",
        "def vocabulary_factory():\n",
        "    \"\"\"Function that create a vocabulary\n",
        "\n",
        "    Default method when a key is not in the dictionary changed to be the\n",
        "    current lenght of the dictionary to provide a unique index for each\n",
        "    new key.\n",
        "\n",
        "    Example:\n",
        "    >> vocab['test']\n",
        "    0\n",
        "    >> vocab['other']\n",
        "    1\n",
        "    >> vocab['test']\n",
        "    0\n",
        "    \"\"\"\n",
        "    vocab = defaultdict()\n",
        "    vocab.default_factory = lambda: len(vocab)\n",
        "    return vocab\n",
        "\n",
        "def word_to_index(corpus: list[list[str]], vocab: defaultdict) -> list[int]:\n",
        "    \"\"\"Function that maps each word in a corpus to a unique index\"\"\"\n",
        "    for sent in corpus:\n",
        "        yield [vocab[word] for word in sent]\n"
      ],
      "metadata": {
        "id": "2rIMEawKzNOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = vocabulary_factory()"
      ],
      "metadata": {
        "id": "j1q_i5t50Yks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexed_sents = list(word_to_index(corpus_train, vocab))"
      ],
      "metadata": {
        "id": "nqkFcoCG0hks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexed_sents[:4]"
      ],
      "metadata": {
        "id": "F2Qf8BAr9LkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El vocabulario aun no esta completo. Debemos agregar etiquetas que indiquen BOS (Beginning Of String) y EOS (End Of String). Debemos añadirlos a cada oración en nuestro corpus de entrenamiento:\n",
        "\n",
        "$$<s> w_1 ... w_k </s>$$\n",
        "\n",
        "De esta forma, podremos obtener probabilidades inciales y transiciones terminales (aquellas que van hacía el símbolo de termino EOS).\n",
        "\n",
        "Estas etiquetas con arbitrarias, usaremos entonces $BOS = <s>$ y $EOS = </s>$"
      ],
      "metadata": {
        "id": "wc4vIpMk9Xne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BOS = \"<s>\"\n",
        "EOS = \"</s>\"\n",
        "\n",
        "BOS_IDX = max(vocab.values())+2\n",
        "EOS_IDX = max(vocab.values())+1\n",
        "\n",
        "vocab[BOS] = BOS_IDX\n",
        "vocab[EOS] = EOS_IDX\n",
        "\n",
        "indexed_corpus_train = [[BOS_IDX] + sent + [EOS_IDX] for sent in indexed_sents]"
      ],
      "metadata": {
        "id": "4Lai8RZ-9xwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_index_to_word(vocab: defaultdict) -> dict:\n",
        "    \"\"\"Map indices as keys and words as values from a vocabulary\"\"\"\n",
        "    return {index: word for word, index in vocab.items()}"
      ],
      "metadata": {
        "id": "kM3ogDYmd25V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_words = get_index_to_word(vocab)"
      ],
      "metadata": {
        "id": "82hIZpkuel1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_words[100]"
      ],
      "metadata": {
        "id": "psxrRpiTesT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "id": "fi6RF6c9tuB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimación del modelo de n-gramas\n",
        "\n",
        "Una vez preprocesadas las cadenas pasaremos a estimar el modelo. Para esta estimación, tomaremos en cuenta dos parámetros:\n",
        "\n",
        "*   El tamaño de n-gramas; es decir, qué tantos elementos previos consideraremos para estimar la probabilidad de que ocurra una palabra.\n",
        "    - bigramas\n",
        "    - trigramas\n",
        "    - etc\n",
        "*   El elemento $\\lambda$ para estimar la probabilidad con smoothing de Lidstone. En ese sentido, dado un n-grama $w_{i-n+1} ... w_{i-1} w_i$ estimaremos la probabilidad como:\n",
        "\n",
        "$$p(w_i|w_{i-1}...w_{i-n+1}) = \\frac{C(w_{i-n+1} ... w_{i-1} w_i) + \\lambda}{C(w_{i-n+1} ... w_{i-1}) + \\lambda N}$$\n",
        "\n",
        "donde $N$ es el tamaño del vocabulario."
      ],
      "metadata": {
        "id": "dRTRRsFEfBxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from itertools import chain\n",
        "\n",
        "def get_n_grams(indexed_sents: list[list[str]], n=2) -> chain:\n",
        "    return chain(*[zip(*[sent[i:] for i in range(n)]) for sent in indexed_sents])\n",
        "\n",
        "def get_model(sents: list[list[str]], vocabulary: defaultdict, n: int=2, l: float=1.0) -> tuple:\n",
        "\n",
        "    # Get n_grams\n",
        "    n_grams = get_n_grams(sents, n)\n",
        "\n",
        "    # Get n_grams frequencies\n",
        "    freq_n_grams = Counter(n_grams)\n",
        "\n",
        "    # Get vocabulary length (without BOS/EOS)\n",
        "    N = len(vocabulary) - 2\n",
        "    # Calculate tensor dimentions for transition probabilities\n",
        "    # For columns (conditional word) we consider the EOS element so we add 1\n",
        "    dim = (N,)*(n-1) + (N+1,)\n",
        "\n",
        "    # Transition tensor\n",
        "    A = np.zeros(dim)\n",
        "    # Initial Probabilities\n",
        "    Pi = np.zeros(N)\n",
        "\n",
        "    for n_gram, frec in freq_n_grams.items():\n",
        "      # Fill the tensor with frequencies\n",
        "      if n_gram[0] != BOS_IDX:\n",
        "          A[n_gram] = frec\n",
        "      # Getting initial frequencies\n",
        "      elif n_gram[0] == BOS_IDX and n_gram[1] != EOS_IDX:\n",
        "          Pi[n_gram[1]] = frec\n",
        "\n",
        "    # Calculating probabilities from frequencies\n",
        "    # We consider the parameter `l` for Lidstone Smoothing\n",
        "    for h, b in enumerate(A):\n",
        "      A[h] = ((b+l).T/(b+l).sum(n-2)).T\n",
        "\n",
        "    # Calculating initial probabilities\n",
        "    Pi = (Pi+l)/(Pi+l).sum(0)\n",
        "\n",
        "    # We get our model\n",
        "    return A, Pi"
      ],
      "metadata": {
        "id": "i4K_12dCfpSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detalles de implementación"
      ],
      "metadata": {
        "id": "ixkjmwd_tZAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigrams = get_n_grams(indexed_corpus_train, n=2)"
      ],
      "metadata": {
        "id": "m5zyB85vg2pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(bigrams)[:3]"
      ],
      "metadata": {
        "id": "9d2UMQ7kvAaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, b in enumerate(bigrams):\n",
        "    print(b)\n",
        "    print(vocab_words[b[0]], vocab_words[b[1]])\n",
        "    if i == 10:\n",
        "        break"
      ],
      "metadata": {
        "id": "YBimPxZLhKhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = len(vocab) - 2\n",
        "n = 3\n",
        "dim = (N,)*(n-1) + (N+1,)"
      ],
      "metadata": {
        "id": "_QgOlnzdqaCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(N,)*(n-1) + (N+1,)"
      ],
      "metadata": {
        "id": "fe83aY2lr9cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimación del modelo"
      ],
      "metadata": {
        "id": "z0o6VlNstvJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estimaremos un modelo de trigramas con $λ = 1$, es decir con smoothing Laplaciano"
      ],
      "metadata": {
        "id": "myM7CJ4RtzG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "trigram_model = get_model(indexed_corpus_train, vocab, n=3, l=1)"
      ],
      "metadata": {
        "id": "eULuLPORtybv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A_trigram = trigram_model[0]\n",
        "print(\"Tensor dimention\", A_trigram.shape)\n",
        "print(\"Suma de probabilidades\")\n",
        "print(A_trigram.sum(1))"
      ],
      "metadata": {
        "id": "dhsfeuTruIA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estimando un modelo de bigramas con $λ = 1$"
      ],
      "metadata": {
        "id": "BaT1LOfuzrzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "bigram_model = get_model(indexed_corpus_train, vocab, n=2, l=1)"
      ],
      "metadata": {
        "id": "FGemNiIHz7KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A_bigram = bigram_model[0]\n",
        "print(\"Tensor dimention\", A_bigram.shape)\n",
        "print(\"Suma de probabilidades\")\n",
        "print(A_bigram.sum(1))"
      ],
      "metadata": {
        "id": "UxUQjgKC0A2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aplicaciones"
      ],
      "metadata": {
        "id": "TLJuNflIyp4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Obtener la probabilidad de una cadena\n",
        "2. Predecir una palabra siguiente\n",
        "3. Generación de texto\n",
        "\n",
        "Para determinar la probabilidad, utilizaremos la función:\n",
        "\n",
        "$$p(w_1 ... w_k) = \\prod_{i=1}^k p(w_i|w_{i-1} ... w_{i-n+1})$$\n",
        "\n",
        "Dado que las cadenas pueden extenderse y las probabilidades son pequeñas, es posible que la probabilidad se haga tan pequeña que aparezca como un cero. Para evitar esto, utilizaremos probabilidad logarítimicada, dada por:\n",
        "\n",
        "$$\\log p(w_1 ... w_k) = \\sum_{i=1}^k \\log p(w_i|w_{i-1} ... w_{i-n+1})$$"
      ],
      "metadata": {
        "id": "sYJiOYJOyrRU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Obtener la probabilidad de una cadena"
      ],
      "metadata": {
        "id": "K3B9M4CUy25I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sent_probability(sentence: str, vocab: defaultdict, model: tuple) -> float:\n",
        "    A, Pi = model\n",
        "    # Getting the n from n-grams\n",
        "    n = len(A.shape)\n",
        "    indexed_sentence = [vocab[word] for word in sentence.split()]\n",
        "    first_indexed_word = indexed_sentence[0]\n",
        "    # Getting initial probability\n",
        "    try:\n",
        "        probability = np.log(Pi[first_indexed_word])\n",
        "    except:\n",
        "        print(f\"[WARN] OOV for word as BOS with index={first_indexed_word}\")\n",
        "        probability = 0.0\n",
        "\n",
        "    # Getting n-grams of the sentence\n",
        "    n_grams = get_n_grams([indexed_sentence], n)\n",
        "    for n_gram in n_grams:\n",
        "        try:\n",
        "          probability += np.log(A[n_gram])\n",
        "        except:\n",
        "          print(f\"[WARN] OOV for n_gram={n_gram}\")\n",
        "          probability += 0.0\n",
        "\n",
        "    return probability"
      ],
      "metadata": {
        "id": "ZFC1pcLay5wY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \" \".join(corpus_train[-1])\n",
        "print(f\"La probabilidad de la cadena: <{sentence}>\")\n",
        "print(f\"\\t\\t Modelo de trigramas: \", np.exp(get_sent_probability(sentence, vocab, trigram_model)))\n",
        "print(f\"\\t\\t Modelo de bigramas: \", np.exp(get_sent_probability(sentence, vocab, bigram_model)))"
      ],
      "metadata": {
        "id": "zZJGky81yO18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = \"and god said\""
      ],
      "metadata": {
        "id": "OFSlmem5CjSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.exp(get_sent_probability(TEST_SENTENCE, vocab, bigram_model))"
      ],
      "metadata": {
        "id": "1tE26z_F6vQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Predecir la palabra siguiente"
      ],
      "metadata": {
        "id": "v8zil9VL_nOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(sentence: str, vocab: defaultdict, vocab_words: dict, model: tuple) -> str:\n",
        "    A, Pi = model\n",
        "    history = len(A.shape) - 1\n",
        "    indexed_sentence = [vocab[word] for word in sentence.split()]\n",
        "    prev_n_gram = tuple(indexed_sentence[-history:])\n",
        "    probability = get_sent_probability(sentence, vocab, model)\n",
        "    next_word = np.argmax(probability + np.log(A[prev_n_gram]))\n",
        "    return vocab_words[next_word]"
      ],
      "metadata": {
        "id": "PEDV7b1r_lgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_next_word(TEST_SENTENCE, vocab, vocab_words, trigram_model)"
      ],
      "metadata": {
        "id": "tYmP3Edg8V34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_next_word(TEST_SENTENCE, vocab, vocab_words, bigram_model)"
      ],
      "metadata": {
        "id": "oFo6wkbzBBIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Generación de texto"
      ],
      "metadata": {
        "id": "ZyIHzE4gBMv4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iterando sobre la función anterior podemos producir texto. Nustro algoritmo buscara el token *EOS* para detenerse o despues de producir *N* tokens."
      ],
      "metadata": {
        "id": "eTx2WHYtBVah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_laguage(sentence: str, vocab: defaultdict, vocab_words: dict, model: tuple, limit: int) -> str:\n",
        "    next_word = \"\"\n",
        "    result = sentence\n",
        "    i = 0\n",
        "    while next_word != \"</s>\":\n",
        "        next_word = predict_next_word(result, vocab, vocab_words, model)\n",
        "        result += \" \" + next_word\n",
        "        i += 1\n",
        "        if i == limit:\n",
        "            break\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "WvjynUgnBHvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Modelo de trigramas: {TEST_SENTENCE}\")\n",
        "generate_laguage(TEST_SENTENCE, vocab, vocab_words, trigram_model, 100)"
      ],
      "metadata": {
        "id": "8fZaWsjLCYWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Modelo de bigramas\")\n",
        "generate_laguage(\"and god\", vocab, vocab_words, bigram_model, 10)"
      ],
      "metadata": {
        "id": "3IPJQKpTCtSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Práctica 7: Evaluación de modelos de lenguaje\n",
        "\n",
        "**Fecha de entrega**: 5 de noviembre de 2023"
      ],
      "metadata": {
        "id": "AO9uvsff3wUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La calidad de un modelo del lenguaje puede ser evaluado por medio de su perplejidad\n",
        "\n",
        "- Investigar como calcular la perplejidad de un modelo del lenguaje y como evaluarlo con esa medida\n",
        "    - Incluir en el `README.md` una sintesis de esta investigación (Un par de parrafos)\n",
        "- Crear un par de modelos del lenguaje usando un **corpus en español**\n",
        "    - Corpus: El Quijote\n",
        "        - URL: https://www.gutenberg.org/ebooks/2000\n",
        "    - Modelo de n-gramas con `n = [2, 3]`\n",
        "    - Hold out con `test = 30%` y `train = 70%`\n",
        "- Evaluar los modelos y reportar la perplejidad de cada modelo\n",
        "  - Comparar los resultados entre los diferentes modelos del lenguaje (bigramas, trigramas)\n",
        "  - ¿Cual fue el modelo mejor evaluado? ¿Porqué?\n"
      ],
      "metadata": {
        "id": "sBdjG6yY4FoA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M_3twxED2nPd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}