{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a65613b-6549-4c2d-9657-8fcbd2acf7c1",
   "metadata": {},
   "source": [
    "# Práctica 7: Modelos del lenguaje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c893ad-265d-42c7-8812-b7bc42fe69d8",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd631c91-d100-4baf-833c-82581bc2062d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Leer archivo y generar lista de oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b22f8a05-e70b-4154-896a-771d29901f1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = './el-quijote.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6faff8f-774d-4cfd-89a1-b3dbf40315d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35522"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "encoding = 'utf-8-sig'\n",
    "\n",
    "try:\n",
    "    with open(path, 'r', encoding=encoding) as f:\n",
    "        contents = f.readlines()\n",
    "        #parsed_corpus = [ ast.literal_eval(x) for x in contents ]\n",
    "except UnicodeDecodeError:\n",
    "    print(\"Error: Unable to decode the file with the specified encoding.\")\n",
    "    \n",
    "len(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e3450ef-2569-484f-8102-12b0af15d872",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['podadera. Frisaba la edad de nuestro hidalgo con los cincuenta años; era de\\n',\n",
       " 'complexión recia, seco de carnes, enjuto de rostro, gran madrugador y amigo\\n',\n",
       " 'de la caza. Quieren decir que tenía el sobrenombre de Quijada, o Quesada,\\n',\n",
       " 'que en esto hay alguna diferencia en los autores que deste caso escriben;\\n',\n",
       " 'aunque, por conjeturas verosímiles, se deja entender que se llamaba\\n',\n",
       " 'Quejana. Pero esto importa poco a nuestro cuento; basta que en la narración\\n',\n",
       " 'dél no se salga un punto de la verdad.\\n',\n",
       " '\\n',\n",
       " 'Es, pues, de saber que este sobredicho hidalgo, los ratos que estaba\\n',\n",
       " 'ocioso, que eran los más del año, se daba a leer libros de caballerías, con\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "839201be-96c0-43ee-92c3-f641296ac25c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "def get_sentences(corpus):\n",
    "    \"\"\"\n",
    "    Recibe una lista de líneas que terminan en '\\n', y genera otra lista que contiene el mismo\n",
    "    texto, pero con '\\n\\n' y '.' como separador.\n",
    "    También se deshace de todos los '...'\n",
    "    \"\"\"\n",
    "    parsed_corpus = [ '<linejump>' if x == '\\n' else x[:-1] for x in contents ]\n",
    "    parsed_corpus = ' '.join(parsed_corpus).split('<linejump>')\n",
    "    parsed_corpus = [x.strip() for x in parsed_corpus]\n",
    "    parsed_corpus = [ x.replace('...','').replace('.', '.<eos>').split('<eos>') for x in parsed_corpus ]\n",
    "    parsed_corpus = reduce(operator.concat, parsed_corpus)\n",
    "    parsed_corpus = filter(lambda x: len(x)>0, parsed_corpus)\n",
    "    parsed_corpus = [x.strip() for x in parsed_corpus]\n",
    "    return parsed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e06fd392-a7f3-4024-a33c-e4fde237dfa7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que vivía un hidalgo de los de lanza en astillero, adarga antigua, rocín flaco y galgo corredor.',\n",
       " 'Una olla de algo más vaca que carnero, salpicón las más noches, duelos y quebrantos los sábados, lantejas los viernes, algún palomino de añadidura los domingos, consumían las tres partes de su hacienda.',\n",
       " 'El resto della concluían sayo de velarte, calzas de velludo para las fiestas, con sus pantuflos de lo mesmo, y los días de entresemana se honraba con su vellorí de lo más fino.',\n",
       " 'Tenía en su casa una ama que pasaba de los cuarenta, y una sobrina que no llegaba a los veinte, y un mozo de campo y plaza, que así ensillaba el rocín como tomaba la podadera.',\n",
       " 'Frisaba la edad de nuestro hidalgo con los cincuenta años; era de complexión recia, seco de carnes, enjuto de rostro, gran madrugador y amigo de la caza.',\n",
       " 'Quieren decir que tenía el sobrenombre de Quijada, o Quesada, que en esto hay alguna diferencia en los autores que deste caso escriben; aunque, por conjeturas verosímiles, se deja entender que se llamaba Quejana.',\n",
       " 'Pero esto importa poco a nuestro cuento; basta que en la narración dél no se salga un punto de la verdad.',\n",
       " 'Es, pues, de saber que este sobredicho hidalgo, los ratos que estaba ocioso, que eran los más del año, se daba a leer libros de caballerías, con tanta afición y gusto, que olvidó casi de todo punto el ejercicio de la caza, y aun la administración de su hacienda.',\n",
       " 'Y llegó a tanto su curiosidad y desatino en esto, que vendió muchas hanegas de tierra de sembradura para comprar libros de caballerías en que leer, y así, llevó a su casa todos cuantos pudo haber dellos; y de todos, ningunos le parecían tan bien como los que compuso el famoso Feliciano de Silva, porque la claridad de su prosa y aquellas entricadas razones suyas le parecían de perlas, y más cuando llegaba a leer aquellos requiebros y cartas de desafíos, donde en muchas partes hallaba escrito: La razón de la sinrazón que a mi razón se hace, de tal manera mi razón enflaquece, que con razón me quejo de la vuestra fermosura.',\n",
       " 'Y también cuando leía: los altos cielos que de vuestra divinidad divinamente con las estrellas os fortifican, y os hacen merecedora del merecimiento que merece la vuestra grandeza.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sentences = get_sentences(contents)\n",
    "corpus_sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649b0190-4533-4aa8-9ca1-766e78bc44ec",
   "metadata": {},
   "source": [
    "### Preprocesar oraciones\n",
    "\n",
    "Pasar todo a minúsculas, y eliminar algunos signos de puntuación (guión largo, comillas, y comillas españolas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "947f5af3-0830-4c78-98bc-aca71a40db55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def delete_substrings(string: str, chars: list) -> str:\n",
    "    final_str = string\n",
    "    for char in chars:\n",
    "        final_str = final_str.replace(char, '')\n",
    "    return final_str\n",
    "\n",
    "def process_sentences(sentences: list[str], stopwords: list) -> list:\n",
    "    clean_sentences = [delete_substrings(sent.lower(), stopwords) for sent in sentences]\n",
    "    # Para que la tokenización funcione necesitamos que los signos de puntuación estén separadas de las palabras.\n",
    "    # Es decir, algo como \"palabra,\" debe estar como \"palabra ,\"\n",
    "    # Haremos esto con el tokenizer de nltk\n",
    "    processed_sentences = [' '.join(word_tokenize(t)) for t in clean_sentences]\n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88679d46-8927-4ba8-b09d-c4b43b4635ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stopwords = [\"''\",\"—\",\"«\",\"»\"]\n",
    "corpus_sentences = process_sentences(corpus_sentences, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39c3c5b3-b104-4e03-8585-2d3a8743172e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['en un lugar de la mancha , de cuyo nombre no quiero acordarme , no ha mucho tiempo que vivía un hidalgo de los de lanza en astillero , adarga antigua , rocín flaco y galgo corredor .',\n",
       " 'una olla de algo más vaca que carnero , salpicón las más noches , duelos y quebrantos los sábados , lantejas los viernes , algún palomino de añadidura los domingos , consumían las tres partes de su hacienda .',\n",
       " 'el resto della concluían sayo de velarte , calzas de velludo para las fiestas , con sus pantuflos de lo mesmo , y los días de entresemana se honraba con su vellorí de lo más fino .',\n",
       " 'tenía en su casa una ama que pasaba de los cuarenta , y una sobrina que no llegaba a los veinte , y un mozo de campo y plaza , que así ensillaba el rocín como tomaba la podadera .',\n",
       " 'frisaba la edad de nuestro hidalgo con los cincuenta años ; era de complexión recia , seco de carnes , enjuto de rostro , gran madrugador y amigo de la caza .',\n",
       " 'quieren decir que tenía el sobrenombre de quijada , o quesada , que en esto hay alguna diferencia en los autores que deste caso escriben ; aunque , por conjeturas verosímiles , se deja entender que se llamaba quejana .',\n",
       " 'pero esto importa poco a nuestro cuento ; basta que en la narración dél no se salga un punto de la verdad .',\n",
       " 'es , pues , de saber que este sobredicho hidalgo , los ratos que estaba ocioso , que eran los más del año , se daba a leer libros de caballerías , con tanta afición y gusto , que olvidó casi de todo punto el ejercicio de la caza , y aun la administración de su hacienda .',\n",
       " 'y llegó a tanto su curiosidad y desatino en esto , que vendió muchas hanegas de tierra de sembradura para comprar libros de caballerías en que leer , y así , llevó a su casa todos cuantos pudo haber dellos ; y de todos , ningunos le parecían tan bien como los que compuso el famoso feliciano de silva , porque la claridad de su prosa y aquellas entricadas razones suyas le parecían de perlas , y más cuando llegaba a leer aquellos requiebros y cartas de desafíos , donde en muchas partes hallaba escrito : la razón de la sinrazón que a mi razón se hace , de tal manera mi razón enflaquece , que con razón me quejo de la vuestra fermosura .',\n",
       " 'y también cuando leía : los altos cielos que de vuestra divinidad divinamente con las estrellas os fortifican , y os hacen merecedora del merecimiento que merece la vuestra grandeza .']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e533bc42-c6ac-4a30-a6c2-c721bfccfa26",
   "metadata": {},
   "source": [
    "### Tokenizar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd12c23-d289-4b87-83e8-f23f7af716c8",
   "metadata": {},
   "source": [
    "Vamos a usar subword tokenization, como se muestra en el notebook de la práctica 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f507eb2-0cc0-4531-a974-ca12399b015a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: subword-nmt in c:\\users\\dell\\miniconda3\\envs\\nlp-environment\\lib\\site-packages (0.3.8)\n",
      "Requirement already satisfied: mock in c:\\users\\dell\\miniconda3\\envs\\nlp-environment\\lib\\site-packages (from subword-nmt) (5.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\miniconda3\\envs\\nlp-environment\\lib\\site-packages (from subword-nmt) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\miniconda3\\envs\\nlp-environment\\lib\\site-packages (from tqdm->subword-nmt) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install subword-nmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a0f8645-6b68-48f2-88e0-a2a428b0c8d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Entrenamos un tokenizador con BPE usando subword-nmt.\n",
    "# Para esto necesitamos ingresarle el texto plano del corpus:\n",
    "\n",
    "def write_plain_text_corpus(raw_text: str, file_name: str) -> None:\n",
    "    with open(f\"{file_name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(raw_text)\n",
    "\n",
    "plain_corpus = '\\n'.join(corpus_sentences)\n",
    "write_plain_text_corpus(plain_corpus, \"plain_processed_quijote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "755cf061-8e80-4545-887f-a21a4a6daed4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 425913\n",
      "types in corpus: 22653\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Primero checamos de qué tamaño es nuestro vocabulario\n",
    "print(\"tokens:\", len(plain_corpus.split()))\n",
    "types_in_corpus = Counter(plain_corpus.split())\n",
    "print(\"types in corpus:\", len(types_in_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35392471-9d5a-4503-b15e-5f9a7a1bfe38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 39209),\n",
       " ('que', 20079),\n",
       " ('y', 17688),\n",
       " ('de', 17540),\n",
       " ('la', 10003),\n",
       " ('a', 9565),\n",
       " ('en', 7967),\n",
       " ('el', 7933),\n",
       " ('.', 7715),\n",
       " ('no', 6126),\n",
       " (';', 4709),\n",
       " ('los', 4630),\n",
       " ('se', 4549),\n",
       " ('con', 4053),\n",
       " ('por', 3785)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types_in_corpus.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6461f42d-bc09-45ed-bb2f-dad7fa31b636",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n",
      "  0%|          | 4/1000 [00:00<00:27, 36.14it/s]\n",
      "  1%|          | 8/1000 [00:00<00:28, 35.10it/s]\n",
      "  1%|1         | 12/1000 [00:00<00:28, 34.28it/s]\n",
      "  2%|1         | 19/1000 [00:00<00:21, 45.32it/s]\n",
      "  3%|2         | 27/1000 [00:00<00:17, 54.43it/s]\n",
      "  4%|3         | 36/1000 [00:00<00:15, 64.05it/s]\n",
      "  5%|4         | 46/1000 [00:00<00:13, 72.55it/s]\n",
      "  6%|5         | 58/1000 [00:00<00:10, 85.87it/s]\n",
      "  7%|6         | 68/1000 [00:01<00:10, 87.20it/s]\n",
      "  8%|8         | 82/1000 [00:01<00:08, 102.09it/s]\n",
      " 11%|#         | 106/1000 [00:01<00:06, 142.28it/s]\n",
      " 12%|#2        | 121/1000 [00:01<00:06, 137.83it/s]\n",
      " 14%|#3        | 136/1000 [00:01<00:06, 140.37it/s]\n",
      " 16%|#6        | 161/1000 [00:01<00:05, 166.59it/s]\n",
      " 18%|#8        | 185/1000 [00:01<00:04, 183.91it/s]\n",
      " 20%|##        | 204/1000 [00:01<00:04, 185.53it/s]\n",
      " 23%|##2       | 229/1000 [00:01<00:03, 204.18it/s]\n",
      " 25%|##5       | 252/1000 [00:01<00:03, 209.77it/s]\n",
      " 27%|##7       | 274/1000 [00:02<00:04, 173.72it/s]\n",
      " 29%|##9       | 294/1000 [00:02<00:03, 180.25it/s]\n",
      " 32%|###2      | 323/1000 [00:02<00:03, 206.90it/s]\n",
      " 35%|###5      | 351/1000 [00:02<00:02, 222.19it/s]\n",
      " 37%|###7      | 374/1000 [00:02<00:02, 220.72it/s]\n",
      " 40%|###9      | 398/1000 [00:02<00:02, 224.80it/s]\n",
      " 43%|####2     | 428/1000 [00:02<00:02, 246.12it/s]\n",
      " 46%|####6     | 460/1000 [00:02<00:02, 267.34it/s]\n",
      " 49%|####9     | 490/1000 [00:02<00:01, 276.75it/s]\n",
      " 52%|#####2    | 521/1000 [00:03<00:01, 284.98it/s]\n",
      " 55%|#####5    | 550/1000 [00:03<00:01, 286.43it/s]\n",
      " 58%|#####8    | 580/1000 [00:03<00:01, 290.00it/s]\n",
      " 61%|######1   | 610/1000 [00:03<00:01, 290.25it/s]\n",
      " 64%|######4   | 645/1000 [00:03<00:01, 304.98it/s]\n",
      " 68%|######7   | 676/1000 [00:03<00:01, 303.63it/s]\n",
      " 71%|#######   | 707/1000 [00:03<00:00, 303.95it/s]\n",
      " 75%|#######4  | 747/1000 [00:03<00:00, 330.22it/s]\n",
      " 78%|#######8  | 781/1000 [00:03<00:00, 331.21it/s]\n",
      " 82%|########1 | 817/1000 [00:03<00:00, 339.09it/s]\n",
      " 86%|########6 | 861/1000 [00:04<00:00, 363.08it/s]\n",
      " 90%|######### | 903/1000 [00:04<00:00, 367.00it/s]\n",
      " 94%|#########4| 945/1000 [00:04<00:00, 378.64it/s]\n",
      " 98%|#########8| 983/1000 [00:04<00:00, 373.57it/s]\n",
      "100%|##########| 1000/1000 [00:04<00:00, 225.12it/s]\n"
     ]
    }
   ],
   "source": [
    "!subword-nmt learn-bpe -s 1000 < plain_processed_quijote.txt > quijote_tokenizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c84f96d4-8f3c-4ab6-bd3d-b09589d02c90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized: @@ camin@@ ando nuestro f@@ l@@ am@@ ante aventu@@ r@@ er@@ o@@  \n"
     ]
    }
   ],
   "source": [
    "tokenized = !echo \"caminando nuestro flamante aventurero\" | subword-nmt apply-bpe -c quijote_tokenizer.model\n",
    "print(\"tokenized:\", list(tokenized)[0].replace('\"',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "283f07e0-96e9-401f-9a37-2ec05573f159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!subword-nmt apply-bpe -c quijote_tokenizer.model < plain_processed_quijote.txt > quijote_tokenized_1k.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "087a3bfc-f64c-45bc-8d47-42978fe6ef6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ahora, con esta tokenización, veremos cuántos tokens y tipos hay\n",
    "\n",
    "with open(\"quijote_tokenized_1k.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    tokenized_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dde48c77-d16f-478a-9942-830486f1ce9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Agregamos EOS y BOS\n",
    "tokenized_text = ['<s> ' + sent + ' </s>' for sent in tokenized_text.split('\\n')]\n",
    "tokenized_text = '\\n'.join(tokenized_text).replace('.','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "610f93fe-4340-4509-a219-57dbc34ca48a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_corpus = tokenized_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53b5fea1-26c6-4e11-a9ce-af6512734dda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 649455\n",
      "types in corpus: 1079\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens:\", len(tokenized_corpus))\n",
    "types_in_corpus = Counter(tokenized_corpus)\n",
    "print(\"types in corpus:\", len(types_in_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97f11e1c-c1de-43d1-9488-2794ed40e7a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 39209),\n",
       " ('que', 20321),\n",
       " ('de', 18185),\n",
       " ('y', 18057),\n",
       " ('a', 11565),\n",
       " ('la', 11521),\n",
       " ('<s>', 9248),\n",
       " ('</s>', 9248),\n",
       " ('en', 9028),\n",
       " ('el', 8152),\n",
       " ('no', 6995),\n",
       " ('los', 5274),\n",
       " ('se', 5223),\n",
       " (';', 4709),\n",
       " ('le', 4348)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types_in_corpus.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9842edfc-70f8-4af4-b168-f5fa1542b34e",
   "metadata": {},
   "source": [
    "Ahora sí, formamos el corpus con las oraciones tokenizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63f33613-b612-48d3-8fb4-f2d3499a86aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_corpus_sentences = [ sent.split() for sent in tokenized_text.split('\\n') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "608ae7c1-a6e7-4e70-8273-cf4235e0245d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'en', 'un', 'lugar', 'de', 'la', 'mancha', ',', 'de', 'cu@@', 'yo', 'nombre', 'no', 'quiero', 'a@@', 'cor@@', 'dar@@', 'me', ',', 'no', 'ha', 'mucho', 'tiempo', 'que', 'vi@@', 'vía', 'un', 'hi@@', 'd@@', 'algo', 'de', 'los', 'de', 'l@@', 'anza', 'en', 'as@@', 'ti@@', 'll@@', 'ero', ',', 'ad@@', 'ar@@', 'ga', 'an@@', 'ti@@', 'gua', ',', 'ro@@', 'c@@', 'í@@', 'n', 'f@@', 'lac@@', 'o', 'y', 'gal@@', 'go', 'cor@@', 're@@', 'dor', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_corpus_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16164c5-44d9-4a1b-a7e0-95fe986b614c",
   "metadata": {},
   "source": [
    "### Diccionario de vocabulario\n",
    "Nuestro modelo del lenguaje requiere que pasemos nuestras palabras a indices numericos. Utilizaremos enteros para estimar el modelo. \n",
    "\n",
    "Crearemos dos diccionarios: \n",
    "1. el primero tomara la palabra y lo convertira a indice (Para acceder a las probabilidades del modelo) \n",
    "2. El segundo tomará los indices y los convertira de vuelta a palabras (Nos ayudará a recuperar las palabras a partir de los índices del modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d652a81a-0ac7-4863-8ee8-9737dcfcee8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index of word 'lugar':   3\n",
      "word in index 3:         lugar\n"
     ]
    }
   ],
   "source": [
    "vocabulary = list(types_in_corpus.keys())\n",
    "vocabulary_by_type = { tipo: indice for (tipo, indice) in zip(vocabulary, range(len(vocabulary)))}\n",
    "vocabulary_by_index = { indice: tipo for (tipo, indice) in zip(vocabulary, range(len(vocabulary)))}\n",
    "\n",
    "print(\"index of word 'lugar':  \", vocabulary_by_type['lugar'])\n",
    "print(\"word in index 3:        \", vocabulary_by_index[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e09ec9d-f65b-4005-aa76-238b92ae8432",
   "metadata": {},
   "source": [
    "### Indexar tokens del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e974a37a-288a-49b2-9abf-9eb76ec08bf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def word_to_index(corpus: list[list[str]], vocab_by_type) -> list[list[int]]:\n",
    "    \"\"\"Function that maps each word in a corpus to a unique index\"\"\"\n",
    "    return [ [vocab_by_type[token] for token in sent] for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba0f9298-aa04-459e-b1b3-4ee91a272755",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indexed_corpus_sentences = word_to_index(tokenized_corpus_sentences, vocabulary_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b11f929b-8981-4bba-9053-194669892439",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 4, 8, 9, 10, 11, 12, 13, 14, 15, 16, 7, 11, 17, 18, 19, 20, 21, 22, 2, 23, 24, 25, 4, 26, 4, 27, 28, 1, 29, 30, 31, 32, 7, 33, 34, 35, 36, 30, 37, 7, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 14, 48, 49, 50]\n"
     ]
    }
   ],
   "source": [
    "print(indexed_corpus_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e594b969-147c-4c1b-bf78-7917c7a69ea1",
   "metadata": {},
   "source": [
    "## Dividir en conjuntos de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f7ee56a-cb43-47b7-92ee-d5e467cc3bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train len: 6473 test len: 2775\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "corpus_train, corpus_test = train_test_split(indexed_corpus_sentences, test_size=0.3)\n",
    "\n",
    "len(corpus_train) + len(corpus_test) == len(indexed_corpus_sentences)\n",
    "\n",
    "print(\"Train len:\", len(corpus_train), \"test len:\", len(corpus_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d490e3-a987-4cbe-8691-20cba58a89cd",
   "metadata": {},
   "source": [
    "## Estimación del modelo de n-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e27b8e9f-3c18-47e2-a7a2-11945ecf2692",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_n_grams(indexed_sents: list[list[str]], n=2) -> chain:\n",
    "    \"\"\"\n",
    "    Returns one iterable with each of the n-grams of each of the sentences in indexed_sents.\n",
    "    \"\"\"\n",
    "    return chain(*[zip(*[sent[i:] for i in range(n)]) for sent in indexed_sents])\n",
    "\n",
    "def get_model(sents: list[list[str]], vocabulary: defaultdict, n: int=2, l: float=1.0) -> tuple:\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        - sents: sentences to train the model with (list of lists of *types indexes*)\n",
    "        - vocabulary: dictionary where the keys are the types (not indexes), with all the possible types in the corpus\n",
    "        - n: number of grams\n",
    "        - l: parameter `l` for Lidstone Smoothing\n",
    "        \n",
    "    Returns a tuple (A, Pi), where A is a tensor with all the trasition probabilities in the\n",
    "    n-grams model, and Pi are the initial probabilities for each type in the vocabulary.\n",
    "    \n",
    "    Dimension of A: (N,)*(n-1) + (N,)\n",
    "    Dimension of Pi: N\n",
    "    \n",
    "    Where N is the size of the vocabulary, and n is the n of n-grams.\n",
    "    \"\"\"\n",
    "    BOS_IDX = vocabulary['<s>']\n",
    "    EOS_IDX = vocabulary['</s>']\n",
    "\n",
    "    # Get n_grams\n",
    "    n_grams = get_n_grams(sents, n)\n",
    "\n",
    "    # Get n_grams frequencies\n",
    "    freq_n_grams = Counter(n_grams)\n",
    "\n",
    "    # Get vocabulary length (WITH BOS/EOS)\n",
    "    N = len(vocabulary)\n",
    "    # Calculate tensor dimentions for transition probabilities\n",
    "    dim = (N,)*(n-1) + (N,)\n",
    "\n",
    "    # Transition tensor\n",
    "    A = np.zeros(dim)\n",
    "    # Initial Probabilities\n",
    "    Pi = np.zeros(N)\n",
    "\n",
    "    for n_gram, frec in freq_n_grams.items():\n",
    "        # Fill the tensor with frequencies\n",
    "        if n_gram[0] != BOS_IDX:\n",
    "            A[n_gram] = frec\n",
    "        # Getting initial frequencies\n",
    "        elif n_gram[0] == BOS_IDX and n_gram[1] != EOS_IDX:\n",
    "            Pi[n_gram[1]] = frec\n",
    "\n",
    "    # Calculating probabilities from frequencies\n",
    "    # We consider the parameter `l` for Lidstone Smoothing\n",
    "    for h, b in enumerate(A):\n",
    "        A[h] = ((b+l).T/(b+l).sum(n-2)).T\n",
    "\n",
    "    # Calculating initial probabilities\n",
    "    Pi = (Pi+l)/(Pi+l).sum(0)\n",
    "\n",
    "    # We get our model\n",
    "    return A, Pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b866406-0821-4fb3-bc13-bd56e33b5a7d",
   "metadata": {},
   "source": [
    "Estimando un modelo de bigramas con  λ=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13f27868-dace-442f-ad11-46444697a277",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "bigram_model = get_model(corpus_train, vocabulary_by_type, n=2, l=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "932ccee0-add1-4635-b7c3-88ebe97f8467",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor dimention (1079, 1079)\n",
      "Suma de probabilidades\n",
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "A_bigram = bigram_model[0]\n",
    "print(\"Tensor dimention\", A_bigram.shape)\n",
    "print(\"Suma de probabilidades\")\n",
    "print(A_bigram.sum(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11301ca-0416-4fdc-b7e9-210292015ca1",
   "metadata": {},
   "source": [
    "Ahora de trigramas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dffea475-4f86-47af-a99e-adb4e0ebc6cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 24.5 s\n",
      "Wall time: 33.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trigram_model = get_model(corpus_train, vocabulary_by_type, n=3, l=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "272b8a8a-e193-44c7-bb0d-9589352eed61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor dimention (1079, 1079, 1079)\n",
      "Suma de probabilidades\n",
      "[[1.         1.         1.         ... 1.         1.         1.        ]\n",
      " [0.99525117 1.01029854 1.00672869 ... 0.99525117 0.99525117 0.99525117]\n",
      " [0.99886128 1.00883412 0.99977286 ... 0.99886128 0.99886128 0.99886128]\n",
      " ...\n",
      " [0.99999914 0.99999914 0.99999914 ... 0.99999914 0.99999914 0.99999914]\n",
      " [0.99999914 0.99999914 0.99999914 ... 0.99999914 0.99999914 0.99999914]\n",
      " [0.99999828 0.99999828 0.99999828 ... 0.99999828 0.99999828 0.99999828]]\n"
     ]
    }
   ],
   "source": [
    "A_trigram = trigram_model[0]\n",
    "print(\"Tensor dimention\", A_trigram.shape)\n",
    "print(\"Suma de probabilidades\")\n",
    "print(A_trigram.sum(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bccfda5-b6a2-45de-a06b-71db7e11ab16",
   "metadata": {},
   "source": [
    "## Aplicando el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a1f7af-32eb-455e-8ce5-1b703370671f",
   "metadata": {},
   "source": [
    "Para determinar la probabilidad, utilizaremos la función:\n",
    "\n",
    "$$p(w_1 ... w_k) = \\prod_{i=1}^k p(w_i|w_{i-1} ... w_{i-n+1})$$\n",
    "\n",
    "Dado que las cadenas pueden extenderse y las probabilidades son pequeñas, es posible que la probabilidad se haga tan pequeña que aparezca como un cero. Para evitar esto, utilizaremos probabilidad logarítimicada, dada por:\n",
    "\n",
    "$$\\log p(w_1 ... w_k) = \\sum_{i=1}^k \\log p(w_i|w_{i-1} ... w_{i-n+1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37f1ebd9-6c20-4d3a-a732-edf704faf4c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sent_probability(indexed_sentence: str, model: tuple) -> float:\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        - indexed_sentence: list of indexes of types\n",
    "        - model: tuple (A, Pi), where A is a tensor with all the trasition probabilities in the n-grams model, and Pi \n",
    "            are the initial probabilities for each type in the vocabulary. This tuple is generated by the get_model function.\n",
    "    \n",
    "    Returns the probability of a sentence (list of indexed tokens), using the formula given in the previous cell.\n",
    "    \"\"\"\n",
    "    A, Pi = model\n",
    "    # Getting the n from n-grams\n",
    "    n = len(A.shape)\n",
    "    first_indexed_word = indexed_sentence[0]\n",
    "    # Getting initial probability\n",
    "    try:\n",
    "        probability = np.log(Pi[first_indexed_word])\n",
    "    except:\n",
    "        print(f\"[WARN] OOV for word as BOS with index={first_indexed_word}\")\n",
    "        probability = 0.0\n",
    "\n",
    "    # Getting n-grams of the sentence\n",
    "    n_grams = get_n_grams([indexed_sentence], n)\n",
    "    for n_gram in n_grams:\n",
    "        try:\n",
    "            probability += np.log(A[n_gram])\n",
    "        except:\n",
    "            print(f\"[WARN] OOV for n_gram={n_gram}\")\n",
    "            probability += 0.0\n",
    "\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "305ed229-bce0-4cc5-992f-1ae9328e1fe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_next_word(indexed_sentence: list[int], model: tuple) -> str:\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        - indexed_sentence: list of indexes of types\n",
    "        - model: tuple (A, Pi), where A is a tensor with all the trasition probabilities in the n-grams model, and Pi \n",
    "            are the initial probabilities for each type in the vocabulary. This tuple is generated by the get_model function.\n",
    "            \n",
    "    Returns the index of the predicted next token (the one with the highest probability) given the previous tokens (the one ins indexed_sentence).\n",
    "    \"\"\"\n",
    "    A, Pi = model\n",
    "    history = len(A.shape) - 1\n",
    "    prev_n_gram = tuple(indexed_sentence[-history:])\n",
    "    probability = get_sent_probability(indexed_sentence, model)\n",
    "    next_word = np.argmax(probability + np.log(A[prev_n_gram]))\n",
    "    return next_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c821ab68-922f-4f28-a5d3-0a0b23ee72c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sentence: ['<s>', 'por', 'ver', 'que', 'tiene', 'este', 'caso', 'un', 'no', 'sé', 'qué', 'de', 'so@@', 'm@@', 'b@@', 'ra', 'de', 'a@@', 'ventura', 'de', 'caballería', ',', 'yo', ',', 'por', 'mi', 'parte', ',', 'os', 'o@@', 'i@@', 'r@@', 'é', ',', 'her@@', 'mano', ',', 'de', 'muy', 'buena', 'g@@', 'ana', ',', 'y', 'así', 'lo', 'har@@', 'án', 'todos', 'estos', 'señores', ',', 'por', 'lo', 'mucho', 'que', 'tienen', 'de', 'discre@@', 'tos', 'y', 'de', 'ser', 'ami@@', 'gos', 'de', 'cu@@', 'ri@@', 'o@@', 'sas', 'no@@', 've@@', 'dad@@', 'es', 'que', 'sus@@', 'pen@@', 'd@@', 'an', ',', 'ale@@', 'gr@@', 'en', 'y', 'entre@@', 'ten@@', 'g@@', 'an', 'los', 's@@', 'enti@@', 'dos', ',', 'como', ',', 'sin', 'duda', ',', 'pien@@', 'so', 'que', 'lo', 'ha', 'de', 'hacer', 'vuestro', 'cu@@']\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = corpus_test[0][:-2]\n",
    "print(\"Test sentence:\", [vocabulary_by_index[i] for i in TEST_SENTENCE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92e0c72f-b011-4a39-973a-9eac4f6c3872",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ant@@'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prueba bigramas\n",
    "next_word_index = predict_next_word(TEST_SENTENCE, bigram_model)\n",
    "vocabulary_by_index[next_word_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e99696fc-ff51-4348-a70b-877d3f23b02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ento'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prueba trigramas\n",
    "next_word_index = predict_next_word(TEST_SENTENCE, trigram_model)\n",
    "vocabulary_by_index[next_word_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1001888-09a4-4a95-989e-e6b592f14fdd",
   "metadata": {},
   "source": [
    "# Perplejidad\n",
    "\n",
    "En el libro de Jurafsky (y en clase) se define la perplejidad como:\n",
    "$$ perplexity(W) = \\sqrt[N]{ \\prod_{i=1}^N \\frac{1}{p(w_i|w_{1} ... w_{i-1})} } $$\n",
    "\n",
    "Donde $W$ es una cadena con el conjunto de prueba (por ejemplo, concatenando todas las cadenas del conjunto de prueba), $N$ es la cantidad de tokens en el conjunto de prueba, y $w_i$ es el *i-ésimo* token en $W$.\n",
    "\n",
    "Para **bigramas** es:\n",
    "$$ perplexity(W) = \\sqrt[N]{ \\prod_{i=1}^N \\frac{1}{p(w_i|w_{i-1})} } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef452249-979d-41c0-8153-04a0dc0b91a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_conditional_prob(w_i, indexed_sentence, model, vocabulary):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - w_i: index (in our vocabulary dictionary) of some token.\n",
    "        - indexed_sentence: list of tokens (or rather, their indexes). This list MUST be of size n-1, where n is the number \n",
    "            of grmas the model uses (the n in n-grams).\n",
    "        - model: tuple (A, Pi), where A is a tensor with all the trasition probabilities in the n-grams model, and Pi \n",
    "            are the initial probabilities for each type in the vocabulary. This tuple is generated by the get_model function.\n",
    "        - vocabulary: dictionary where the keys are the types (not indexes), with all the possible types in the corpus\n",
    "        \n",
    "    Returns the probability (according to the given model) of token w_i given that the previous tokens were the ones in\n",
    "    indexed_sentence.\n",
    "    \"\"\"\n",
    "    A, Pi = model\n",
    "    # Getting the n from n-grams\n",
    "    n = len(A.shape)\n",
    "    prev = indexed_sentence\n",
    "    # Getting n-grams of the sentence\n",
    "    prev.append(w_i)\n",
    "    n_grams = tuple(prev)\n",
    "    if n_grams[0]==vocabulary['</s>'] and n_grams[1]==vocabulary['<s>']:\n",
    "        return 1\n",
    "    try:\n",
    "        probability = A[n_grams]\n",
    "    except:\n",
    "        print(f\"[WARN] OOV for n_gram={n_grams}\")\n",
    "        probability = 0.0\n",
    "    \n",
    "    return probability\n",
    "    \n",
    "\n",
    "def get_prerplexity(W, n, model, vocabulary):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - W is a list of tokens. It's all the sentences in the test set concatenated.\n",
    "        - n is the number of grmas the model uses (the n in n-grams)\n",
    "        - model: tuple (A, Pi), where A is a tensor with all the trasition probabilities in the n-grams model, and Pi \n",
    "            are the initial probabilities for each type in the vocabulary. This tuple is generated by the get_model function.\n",
    "        - vocabulary: dictionary where the keys are the types (not indexes), with all the possible types in the corpus\n",
    "    \n",
    "    Returns the perplexity (as defined previously) of the model for test set W.\n",
    "    \"\"\"\n",
    "    N = len(W) \n",
    "    probs = [1/get_conditional_prob(W[i], W[i-(n-1): i], model, vocabulary) for i in range(n-1, N)]\n",
    "    product = math.prod(probs)\n",
    "    return product**(1/N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c277b7d1-e01c-4ea7-8a55-14b2d2a805d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Perplejidad logarítmica\n",
    "En las diapositivas, la profesora nos mostró la perplejidad logarítmica:\n",
    "$$ \\log(Perplexity(W)) = -\\frac{1}{N} \\sum_{i=1}^N \\log_2{p(w_i|w_{1} ... w_{i-1})}  $$\n",
    "De donde:\n",
    "$$logPerplexity(W) = 2^{-\\frac{1}{N} \\sum_{i=1}^{N} \\log_2 P(w_i | w_1, w_2, \\ldots, w_{i-1})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac63f79c-d084-46a9-965c-51d311b5042d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_log_perplexity(W, n, model, vocabulary):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - W is a list of tokens. It's all the sentences in the test set concatenated.\n",
    "        - n is the number of grmas the model uses (the n in n-grams)\n",
    "        - model: tuple (A, Pi), where A is a tensor with all the trasition probabilities in the n-grams model, and Pi \n",
    "            are the initial probabilities for each type in the vocabulary. This tuple is generated by the get_model function.\n",
    "        - vocabulary: dictionary where the keys are the types (not indexes), with all the possible types in the corpus\n",
    "    \n",
    "    Returns the log perplexity (as defined previously) of the model for test set W.\n",
    "    \"\"\"\n",
    "    N = len(W)\n",
    "    log_probs = [math.log(get_conditional_prob(W[i], W[i-(n-1): i], model, vocabulary), 2) for i in range(n-1, N)]\n",
    "    product = sum(log_probs) * -1/N\n",
    "    return 2**(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a184136-8af4-40ae-8c32-b4d70c1d5423",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35500"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatened_test_set = list(chain(*corpus_test[:500]))\n",
    "len(flatened_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4856047b-6098-4b73-8def-060d22d08b8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplejidad del modelo de bigramas: 119.75132838700031\n"
     ]
    }
   ],
   "source": [
    "perplexity = get_log_perplexity(flatened_test_set, 2, bigram_model, vocabulary_by_type)\n",
    "print(\"Perplejidad del modelo de bigramas:\", perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "04114d3b-8539-4443-ba0c-07c56d84db5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplejidad del modelo de bigramas: 356.80767789670006\n"
     ]
    }
   ],
   "source": [
    "perplexity_3 = get_log_perplexity(flatened_test_set, 3, trigram_model, vocabulary_by_type)\n",
    "print(\"Perplejidad del modelo de bigramas:\", perplexity_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
